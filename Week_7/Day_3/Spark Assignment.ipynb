{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36b9a15",
   "metadata": {},
   "source": [
    "Create a Logistic regression and a Random Forest Classifier to predict whether passengers would survive the demise of the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929abead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/02 01:25:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032c6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ab11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"titanic_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954e5e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7555491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b5ba49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: float (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- SibSp: float (nullable = true)\n",
      " |-- Parch: float (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert longs to floats\n",
    "\n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['PassengerId', 'Survived', 'Pclass','Age', 'SibSp', 'Parch', 'Fare']\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76122606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|Survived|\n",
      "+-----------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+--------+\n",
      "|        1.0|   3.0|Braund, Mr. Owen ...|  male|22.0|  1.0|  0.0|A/5 21171|   7.25| null|       S|     0.0|\n",
      "|        2.0|   1.0|Cumings, Mrs. Joh...|female|38.0|  1.0|  0.0| PC 17599|71.2833|  C85|       C|     1.0|\n",
      "+-----------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Survived']\n",
    "df = df.select(COLUMNS)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b56793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34dae337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    491\n",
       "1.0    216\n",
       "2.0    184\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6690d797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    608\n",
       "1.0    209\n",
       "2.0     28\n",
       "4.0     18\n",
       "3.0     16\n",
       "8.0      7\n",
       "5.0      5\n",
       "Name: SibSp, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['SibSp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65bc4bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    678\n",
       "1.0    118\n",
       "2.0     80\n",
       "5.0      5\n",
       "3.0      5\n",
       "4.0      4\n",
       "6.0      1\n",
       "Name: Parch, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Parch'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8885e8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347082      7\n",
       "CA. 2343    7\n",
       "1601        7\n",
       "3101295     6\n",
       "CA 2144     6\n",
       "           ..\n",
       "9234        1\n",
       "19988       1\n",
       "2693        1\n",
       "PC 17612    1\n",
       "370376      1\n",
       "Name: Ticket, Length: 681, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef4c737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B96 B98        4\n",
       "G6             4\n",
       "C23 C25 C27    4\n",
       "C22 C26        3\n",
       "F33            3\n",
       "              ..\n",
       "E34            1\n",
       "C7             1\n",
       "C54            1\n",
       "E36            1\n",
       "C148           1\n",
       "Name: Cabin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Cabin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32325985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e041716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df.filter(df.Parch != 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123c4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|Survived|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+--------+\n",
      "|          0|     0|   0|  0|177|    0|    0|     0|   0|  686|       2|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_remove.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_remove.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a496b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df_remove.drop(*('PassengerId', 'Name', 'Ticket', 'Cabin'))\n",
    "df_remove = df_remove.filter(df_remove.Age. isNotNull())\n",
    "df_remove = df_remove.filter(df_remove.Embarked. isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "214e814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b66f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- SibSp: float (nullable = true)\n",
      " |-- Parch: float (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_remove.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b5a9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "CATE_FEATURES = ['Sex', 'Embarked']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00d86bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "#label_stringIdx =  StringIndexer(inputCol=\"Survived\", outputCol=\"newSurvived\")\n",
    "#stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b47d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTI_FEATURES = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2882f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cd20db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pclass=3.0, Sex='male', Age=22.0, SibSp=1.0, Parch=0.0, Fare=7.25, Embarked='S', Survived=0.0, SexIndex=0.0, SexclassVec=SparseVector(1, {0: 1.0}), EmbarkedIndex=0.0, EmbarkedclassVec=SparseVector(2, {0: 1.0}), features=DenseVector([1.0, 1.0, 0.0, 3.0, 22.0, 1.0, 0.0, 7.25]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a8db29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pclass=3.0, Sex='male', Age=22.0, SibSp=1.0, Parch=0.0, Fare=7.25, Embarked='S', Survived=0.0, SexIndex=0.0, SexclassVec=SparseVector(1, {0: 1.0}), EmbarkedIndex=0.0, EmbarkedclassVec=SparseVector(2, {0: 1.0}), features=DenseVector([1.0, 1.0, 0.0, 3.0, 22.0, 1.0, 0.0, 7.25])),\n",
       " Row(Pclass=1.0, Sex='female', Age=38.0, SibSp=1.0, Parch=0.0, Fare=71.2833023071289, Embarked='C', Survived=1.0, SexIndex=1.0, SexclassVec=SparseVector(1, {}), EmbarkedIndex=1.0, EmbarkedclassVec=SparseVector(2, {1: 1.0}), features=DenseVector([0.0, 0.0, 1.0, 1.0, 38.0, 1.0, 0.0, 71.2833])),\n",
       " Row(Pclass=3.0, Sex='female', Age=26.0, SibSp=0.0, Parch=0.0, Fare=7.925000190734863, Embarked='S', Survived=1.0, SexIndex=1.0, SexclassVec=SparseVector(1, {}), EmbarkedIndex=0.0, EmbarkedclassVec=SparseVector(2, {0: 1.0}), features=SparseVector(8, {1: 1.0, 3: 3.0, 4: 26.0, 7: 7.925}))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d5acf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+-------+--------+--------+--------+-------------+-------------+----------------+--------------------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Survived|SexIndex|  SexclassVec|EmbarkedIndex|EmbarkedclassVec|            features|\n",
      "+------+------+----+-----+-----+-------+--------+--------+--------+-------------+-------------+----------------+--------------------+\n",
      "|   3.0|  male|22.0|  1.0|  0.0|   7.25|       S|     0.0|     0.0|(1,[0],[1.0])|          0.0|   (2,[0],[1.0])|[1.0,1.0,0.0,3.0,...|\n",
      "|   1.0|female|38.0|  1.0|  0.0|71.2833|       C|     1.0|     1.0|    (1,[],[])|          1.0|   (2,[1],[1.0])|[0.0,0.0,1.0,1.0,...|\n",
      "|   3.0|female|26.0|  0.0|  0.0|  7.925|       S|     1.0|     1.0|    (1,[],[])|          0.0|   (2,[0],[1.0])|(8,[1,3,4,7],[1.0...|\n",
      "|   1.0|female|35.0|  1.0|  0.0|   53.1|       S|     1.0|     1.0|    (1,[],[])|          0.0|   (2,[0],[1.0])|[0.0,1.0,0.0,1.0,...|\n",
      "|   3.0|  male|35.0|  0.0|  0.0|   8.05|       S|     0.0|     0.0|(1,[0],[1.0])|          0.0|   (2,[0],[1.0])|[1.0,1.0,0.0,3.0,...|\n",
      "+------+------+----+-----+-----+-------+--------+--------+--------+-------------+-------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2b2af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"Survived\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71041ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|            features|\n",
      "+--------+--------------------+\n",
      "|     0.0|[1.0,1.0,0.0,3.0,...|\n",
      "|     1.0|[0.0,0.0,1.0,1.0,...|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"Survived\", \"features\"])\n",
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d234d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a7b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|Survived|count(Survived)|\n",
      "+--------+---------------+\n",
      "|     0.0|            328|\n",
      "|     1.0|            229|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby('Survived').agg({'Survived': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a10018",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41fed9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/02 01:34:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/02 01:34:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.9908522767656833,-0.1621320421717106,0.2612724694646815,-0.33985875701556506,-0.005834849713198188,-0.054528842349197915,0.05348017163209499,0.002285740351778472]\n",
      "Intercept: 1.1697834319184208\n"
     ]
    }
   ],
   "source": [
    "# Import `LogisticRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"Survived\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bafcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d79cb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9836b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|Survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|     0.0|       1.0|[0.48865449664423...|\n",
      "|     0.0|       1.0|[0.43619450519124...|\n",
      "|     0.0|       0.0|[0.51581049682227...|\n",
      "|     0.0|       0.0|[0.53009808020190...|\n",
      "|     0.0|       1.0|[0.49699092271870...|\n",
      "|     0.0|       0.0|[0.51431922277506...|\n",
      "|     0.0|       0.0|[0.52600327491420...|\n",
      "|     0.0|       0.0|[0.54008710503582...|\n",
      "|     0.0|       0.0|[0.54521598269143...|\n",
      "|     0.0|       0.0|[0.53579731894507...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"Survived\", \"prediction\", \"probability\")\n",
    "selected.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "394ce176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|Survived|count(Survived)|\n",
      "+--------+---------------+\n",
      "|     0.0|             95|\n",
      "|     1.0|             59|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = predictions.select(\"Survived\", \"prediction\")\n",
    "cm.groupby('Survived').agg({'Survived': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a475e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|              115|\n",
      "|       1.0|               39|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c35bf715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662337662337663"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.filter(cm.Survived == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718be231",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55196e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `RandomForestClassifier`\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Initialize `lr`\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\",\n",
    "                        featuresCol=\"features\")\n",
    "\n",
    "# Fit the data to the model\n",
    "rfModel = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e81aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRF = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f94a8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmRF = predictionsRF.select(\"Survived\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59f0e202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7987012987012987"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmRF.filter(cmRF.Survived == cmRF.prediction).count() / cmRF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218551f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
