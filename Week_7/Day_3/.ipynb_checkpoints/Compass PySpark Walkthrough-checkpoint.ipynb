{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f926137",
   "metadata": {},
   "source": [
    "# Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9a0073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/01 22:41:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5517535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums= sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8d7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b310ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print('%i ' % (num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56276d1",
   "metadata": {},
   "source": [
    "# SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9707833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca163cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the list of tuples with the information\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "\n",
    "# 2. Build the RDD\n",
    "rdd = sc.parallelize(list_p)\n",
    "\n",
    "# 3. Convert to Tuples\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "\n",
    "# 4. Create a DataFrame context\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f59e9519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 19|\n",
      "|Smith| 29|\n",
      "| Adam| 35|\n",
      "|Henry| 50|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_ppl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7ca32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbc3a0",
   "metadata": {},
   "source": [
    "# Machine Learning Example with PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee309cf0",
   "metadata": {},
   "source": [
    "The following are the steps to build a Machine Learning program with PySpark:\n",
    "\n",
    "1. Basic operation with PySpark\n",
    "2. Data preprocessing\n",
    "3. Build a data processing pipeline\n",
    "4. Build the classifier: logistic\n",
    "5. Train and evaluate the model\n",
    "6. Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66608b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: long (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: long (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: long (nullable = true)\n",
      " |-- capital_loss: long (nullable = true)\n",
      " |-- hours_week: long (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/sadhana1002/PredictingSalaryClass-Classification/master/adult.csv\"\n",
    "df = sqlContext.createDataFrame(pd.read_csv(url, \n",
    "                                      names=['Age','workclass',\n",
    "                                             'fnlwgt','education',\n",
    "                                             'education_num',\n",
    "                                             'marital',\n",
    "                                             'occupation',\n",
    "                                             'relationship','race',\n",
    "                                             'sex','capital_gain',\n",
    "                                             'capital_loss',\n",
    "                                             'hours_week',\n",
    "                                             'native_country','label']))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84773d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|Age|workclass        |fnlwgt|education |education_num|marital            |occupation        |relationship  |race  |sex    |capital_gain|capital_loss|hours_week|native_country|label |\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|39 | State-gov       |77516 | Bachelors|13           | Never-married     | Adm-clerical     | Not-in-family| White| Male  |2174        |0           |40        | United-States| <=50K|\n",
      "|50 | Self-emp-not-inc|83311 | Bachelors|13           | Married-civ-spouse| Exec-managerial  | Husband      | White| Male  |0           |0           |13        | United-States| <=50K|\n",
      "|38 | Private         |215646| HS-grad  |9            | Divorced          | Handlers-cleaners| Not-in-family| White| Male  |0           |0           |40        | United-States| <=50K|\n",
      "|53 | Private         |234721| 11th     |7            | Married-civ-spouse| Handlers-cleaners| Husband      | Black| Male  |0           |0           |40        | United-States| <=50K|\n",
      "|28 | Private         |338409| Bachelors|13           | Married-civ-spouse| Prof-specialty   | Wife         | Black| Female|0           |0           |40        | Cuba         | <=50K|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbc5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert longs to floats\n",
    "\n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecb4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|  fnlwgt|\n",
      "+----+--------+\n",
      "|39.0| 77516.0|\n",
      "|50.0| 83311.0|\n",
      "|38.0|215646.0|\n",
      "|53.0|234721.0|\n",
      "|28.0|338409.0|\n",
      "+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a463a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    education|count|\n",
      "+-------------+-----+\n",
      "|    Preschool|   51|\n",
      "|      1st-4th|  168|\n",
      "|      5th-6th|  333|\n",
      "|    Doctorate|  413|\n",
      "|         12th|  433|\n",
      "|          9th|  514|\n",
      "|  Prof-school|  576|\n",
      "|      7th-8th|  646|\n",
      "|         10th|  933|\n",
      "|   Assoc-acdm| 1067|\n",
      "|         11th| 1175|\n",
      "|    Assoc-voc| 1382|\n",
      "|      Masters| 1723|\n",
      "|    Bachelors| 5355|\n",
      "| Some-college| 7291|\n",
      "|      HS-grad|10501|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:=================================>                   (127 + 13) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3975727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "|summary|               age|   workclass|            fnlwgt|    education|     education_num|  marital|       occupation|relationship|               race|    sex|      capital_gain|      capital_loss|        hours_week|native_country| label|\n",
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "|  count|             32561|       32561|             32561|        32561|             32561|    32561|            32561|       32561|              32561|  32561|             32561|             32561|             32561|         32561| 32561|\n",
      "|   mean| 38.58164675532078|        null|189778.36651208502|         null|  10.0806793403151|     null|             null|        null|               null|   null|1077.6488437087312|   87.303829734959|40.437455852092995|          null|  null|\n",
      "| stddev|13.640432553581343|        null| 105549.9776970223|         null|2.5727203320673877|     null|             null|        null|               null|   null| 7385.292084840335|402.96021864899984|12.347428681731847|          null|  null|\n",
      "|    min|              17.0|           ?|           12285.0|         10th|               1.0| Divorced|                ?|     Husband| Amer-Indian-Eskimo| Female|               0.0|               0.0|               1.0|             ?| <=50K|\n",
      "|    max|              90.0| Without-pay|         1484705.0| Some-college|              16.0|  Widowed| Transport-moving|        Wife|              White|   Male|           99999.0|            4356.0|              99.0|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ce1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital_gain|\n",
      "+-------+------------------+\n",
      "|  count|             32561|\n",
      "|   mean|1077.6488437087312|\n",
      "| stddev| 7385.292084840335|\n",
      "|    min|               0.0|\n",
      "|    max|           99999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital_gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99eb84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|age_label| <=50K| >50K|\n",
      "+---------+------+-----+\n",
      "|     17.0|   395|    0|\n",
      "|     18.0|   550|    0|\n",
      "|     19.0|   710|    2|\n",
      "|     20.0|   753|    0|\n",
      "|     21.0|   717|    3|\n",
      "|     22.0|   752|   13|\n",
      "|     23.0|   865|   12|\n",
      "|     24.0|   767|   31|\n",
      "|     25.0|   788|   53|\n",
      "|     26.0|   722|   63|\n",
      "|     27.0|   754|   81|\n",
      "|     28.0|   748|  119|\n",
      "|     29.0|   679|  134|\n",
      "|     30.0|   690|  171|\n",
      "|     31.0|   705|  183|\n",
      "|     32.0|   639|  189|\n",
      "|     33.0|   684|  191|\n",
      "|     34.0|   643|  243|\n",
      "|     35.0|   659|  217|\n",
      "|     36.0|   635|  263|\n",
      "+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age', 'label').sort(\"age_label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9bf0ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_week',\n",
       " 'native_country',\n",
       " 'label']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('education_num').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f762bfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13443"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc9eee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             marital| avg(capital_gain)|\n",
      "+--------------------+------------------+\n",
      "|             Widowed| 571.0715005035247|\n",
      "| Married-spouse-a...| 653.9832535885167|\n",
      "|   Married-AF-spouse| 432.6521739130435|\n",
      "|  Married-civ-spouse|1764.8595085470085|\n",
      "|            Divorced| 728.4148098131893|\n",
      "|       Never-married|376.58831788823363|\n",
      "|           Separated| 535.5687804878049|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital').agg({'capital_gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61297797",
   "metadata": {},
   "source": [
    "# Step 2: Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7542efde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "#age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\", col(\"age\")**2)      # apparently only this line of code is needed.\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d879da26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=39.0, age_square=1521.0, workclass=' State-gov', fnlwgt=77516.0, education=' Bachelors', education_num=13.0, marital=' Never-married', occupation=' Adm-clerical', relationship=' Not-in-family', race=' White', sex=' Male', capital_gain=2174.0, capital_loss=0.0, hours_week=40.0, native_country=' United-States', label=' <=50K')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country', 'label']\n",
    "df = df.select(COLUMNS)\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d5b9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native_country|count(native_country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|            Scotland|                   12|\n",
      "|             Hungary|                   13|\n",
      "|            Honduras|                   13|\n",
      "| Outlying-US(Guam...|                   14|\n",
      "|          Yugoslavia|                   16|\n",
      "|                Laos|                   18|\n",
      "|            Thailand|                   18|\n",
      "|     Trinadad&Tobago|                   19|\n",
      "|            Cambodia|                   19|\n",
      "|                Hong|                   20|\n",
      "|             Ireland|                   24|\n",
      "|             Ecuador|                   28|\n",
      "|              Greece|                   29|\n",
      "|              France|                   29|\n",
      "|                Peru|                   31|\n",
      "|           Nicaragua|                   34|\n",
      "|            Portugal|                   37|\n",
      "|                Iran|                   43|\n",
      "|               Haiti|                   44|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.native_country == 'Holand-Netherlands').count()\n",
    "df.groupby('native_country').agg({'native_country': 'count'}).sort(asc(\"count(native_country)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba418246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df.filter(df.native_country != 'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39b30d",
   "metadata": {},
   "source": [
    "# Step 3: Build a data processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41573e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab9ae533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "| age|age_square|        workclass| fnlwgt| education|education_num|            marital|      occupation|  relationship|  race|  sex|capital_gain|capital_loss|hours_week|native_country| label|workclass_encoded|workclass_vec|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "|39.0|    1521.0|        State-gov|77516.0| Bachelors|         13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|      40.0| United-States| <=50K|              4.0|(9,[4],[1.0])|\n",
      "|50.0|    2500.0| Self-emp-not-inc|83311.0| Bachelors|         13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|         0.0|         0.0|      13.0| United-States| <=50K|              1.0|(9,[1],[1.0])|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\").fit(indexed)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a30dcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical data (same as above, but loop over all categorical features)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "#CONTI_FEATURES  = ['age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']    (for reference)\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35dfae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b7131c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add continuous variable\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efad9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24757458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54068120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/01 22:42:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, age_square=1521.0, workclass=' State-gov', fnlwgt=77516.0, education=' Bachelors', education_num=13.0, marital=' Never-married', occupation=' Adm-clerical', relationship=' Not-in-family', race=' White', sex=' Male', capital_gain=2174.0, capital_loss=0.0, hours_week=40.0, native_country=' United-States', label=' <=50K', workclassIndex=4.0, workclassclassVec=SparseVector(8, {4: 1.0}), educationIndex=2.0, educationclassVec=SparseVector(15, {2: 1.0}), maritalIndex=1.0, maritalclassVec=SparseVector(6, {1: 1.0}), occupationIndex=3.0, occupationclassVec=SparseVector(14, {3: 1.0}), relationshipIndex=1.0, relationshipclassVec=SparseVector(5, {1: 1.0}), raceIndex=0.0, raceclassVec=SparseVector(4, {0: 1.0}), sexIndex=0.0, sexclassVec=SparseVector(1, {0: 1.0}), native_countryIndex=0.0, native_countryclassVec=SparseVector(41, {0: 1.0}), newlabel=0.0, features=SparseVector(100, {4: 1.0, 10: 1.0, 24: 1.0, 32: 1.0, 44: 1.0, 48: 1.0, 52: 1.0, 53: 1.0, 94: 39.0, 95: 77516.0, 96: 2174.0, 97: 13.0, 99: 40.0}))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f8fe0",
   "metadata": {},
   "source": [
    "# Step 4: Build the classifier: logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cc50d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d574b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,1.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "731b7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6af6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       19845|\n",
      "|  1.0|        6300|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "042d1aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        4875|\n",
      "|  1.0|        1541|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c93ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/01 22:48:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/01 22:48:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e0dcca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.0809250866528022,-0.129507147468114,-0.08150398087537912,-0.16920463822399695,-0.1398426935211459,0.16929665899340623,0.1944166970906636,-0.5915595984201846,-0.1925889394358705,-0.0805444510720634,0.21383633873683838,0.3983502764322131,-0.007143521372852706,-0.3051585346419398,-0.02975883437550814,-0.3527048207304107,-0.4286635432324258,0.5457909895653659,-0.3906038128582779,-0.18336682389099498,0.6450330629495782,-0.40134773327284035,-0.38299158281207907,0.3204233869817902,-0.3497134607608081,-0.20065596019715815,-0.24448344527598903,-0.14426259221652873,-0.17667670054673706,0.17399311013247068,-0.058608833251681036,0.2916733109643875,-0.11122939022514504,0.043806409459561005,-0.2938610063466801,-0.226404107877248,-0.17069506892688838,-0.148035071579981,-0.2967350153317239,-0.3422495832938999,0.15399777334714967,0.14993550649100207,-0.32071273825674057,0.2620836043507481,-0.1902456007666155,-0.2999374790257608,-0.2508204027572286,0.43045399989136307,-0.07525790075866079,-0.15442529545177902,-0.11005694999009911,-0.2902933930241263,0.17556777152311462,-0.12839855475957235,-0.3633430077135738,-0.18310601115298075,0.015350720014346122,-0.05258914495638626,0.03293979171392108,-0.2901157318790495,-0.25973373995348864,-0.11201020144972719,-0.14136272024733018,-0.04925493851463711,-0.19870756432430375,-0.34887466677936335,-0.31599165296695414,0.012232976882384335,-0.4590257173127538,-0.3588573767581429,-0.2269246252793201,-0.019137146512140456,-0.3160022860911802,-0.5366199059443297,-0.09863056002994305,-0.3269594673518113,-0.08011691219066897,-0.3569647500900546,-0.5492498905382781,-0.39906343849626486,-0.035167741040667916,-0.3133096964687569,-0.4166773867247927,0.04819330139779972,-0.27580167163937347,0.3081347094935461,-0.2808166360255259,-0.3235096603340496,-0.3607468044316025,0.2615131143600774,-0.6508411257235162,-0.24622191158288478,0.18639436600242237,-0.2829279876659968,0.006715965923423805,1.6145010370331027e-08,2.2405890460928968e-05,0.025668855620993306,0.00022505510909613465,0.008508150106911257]\n",
      "Intercept: -1.9056027574894725\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001de3cf",
   "metadata": {},
   "source": [
    "# Step 5: Train and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dff88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b1838b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d900182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 114:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.67264667156572...|\n",
      "|  0.0|       0.0|[0.74767042053991...|\n",
      "|  0.0|       0.0|[0.84128229944305...|\n",
      "|  0.0|       0.0|[0.82426786691640...|\n",
      "|  0.0|       0.0|[0.75612484924238...|\n",
      "|  0.0|       0.0|[0.73064218526286...|\n",
      "|  0.0|       0.0|[0.81338323719044...|\n",
      "|  0.0|       0.0|[0.75233435774951...|\n",
      "|  0.0|       0.0|[0.86637068175599...|\n",
      "|  0.0|       0.0|[0.79559826017480...|\n",
      "|  0.0|       0.0|[0.84893801103873...|\n",
      "|  0.0|       0.0|[0.86364509835917...|\n",
      "|  0.0|       0.0|[0.85151722042409...|\n",
      "|  0.0|       0.0|[0.88071263315657...|\n",
      "|  0.0|       0.0|[0.88299907705955...|\n",
      "|  0.0|       0.0|[0.64802095182998...|\n",
      "|  0.0|       1.0|[0.32452120454135...|\n",
      "|  0.0|       0.0|[0.67247236341054...|\n",
      "|  0.0|       0.0|[0.88865543922044...|\n",
      "|  0.0|       0.0|[0.84328923204674...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "450c8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92a9779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        4875|\n",
      "|  1.0|        1541|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e279cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|             5788|\n",
      "|       1.0|              628|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2cddf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8243453865336658"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.filter(cm.label == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f7d698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 82.435%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7cf88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8872411188206126\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64898ce1",
   "metadata": {},
   "source": [
    "# Step 6 (Stretch): Tune the hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fbc72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "223991e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 50.447 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "220fc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 552:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 84.991%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "720acd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_e589d6b0ea7d', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_e589d6b0ea7d', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22201e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
