{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NumberGAN_01.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMGUy0q0NzS9HCI2AGbiAq1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NJp-D51g0IDd"},"source":["## **1) Importing Python Packages for GAN**\n"]},{"cell_type":"code","metadata":{"id":"1k5mFBuzzl2a","executionInfo":{"status":"ok","timestamp":1631738918653,"user_tz":240,"elapsed":6807,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}}},"source":["from keras.datasets import mnist\n","from keras.models import Sequential, Model\n","from keras.layers import BatchNormalization\n","from keras.layers import Dense, Reshape, Flatten, Input\n","from keras.layers.advanced_activations import LeakyReLU\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","!mkdir generated_images"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yr-eZOzg0X79"},"source":["## **2) Parameters for Neural Networks & Data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RThZMDruz9cB","executionInfo":{"status":"ok","timestamp":1631738918653,"user_tz":240,"elapsed":6,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}},"outputId":"9e778f43-6322-4e3d-8caf-3aae16daa67c"},"source":["img_width = 28\n","img_height = 28\n","channels = 1\n","img_shape = (img_width, img_height, channels)\n","latent_dim = 100\n","adam = Adam(lr=0.0001)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"]}]},{"cell_type":"markdown","metadata":{"id":"U3bcJZZg0cqy"},"source":["## **3) Building Generator**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdiqZpri0iQh","executionInfo":{"status":"ok","timestamp":1631738922484,"user_tz":240,"elapsed":3834,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}},"outputId":"91df8b1f-5b07-4ab0-f2e6-7bb28b77202c"},"source":["def build_generator():\n","  model = Sequential()\n","\n","  model.add(Dense(256, input_dim=latent_dim))\n","  model.add(LeakyReLU(alpha=0.2))\n","  model.add(BatchNormalization(momentum=0.8))\n","  model.add(Dense(512))\n","  model.add(LeakyReLU(alpha=0.2))\n","  model.add(BatchNormalization(momentum=0.8))\n","  model.add(Dense(1024))\n","  model.add(LeakyReLU(alpha=0.2))\n","  model.add(BatchNormalization(momentum=0.8))\n","  model.add(Dense(np.prod(img_shape), activation='tanh'))\n","  model.add(Reshape(img_shape))\n","  \n","  model.summary()\n","  return model\n","\n","generator = build_generator()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 256)               25856     \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 256)               1024      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 1024)              0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 1024)              4096      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 784)               803600    \n","_________________________________________________________________\n","reshape (Reshape)            (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 1,493,520\n","Trainable params: 1,489,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bt6QsJCW0mcI"},"source":["## **4) Building Discriminator**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2JzEAPv0lKt","executionInfo":{"status":"ok","timestamp":1631738924579,"user_tz":240,"elapsed":87,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}},"outputId":"a6e5c6a2-07fd-4a19-edf7-730373636fe4"},"source":["def build_discriminator():\n","  model = Sequential()\n","\n","  model.add(Flatten(input_shape=img_shape))\n","  model.add(Dense(512))\n","  model.add(LeakyReLU(alpha=0.2))\n","  model.add(Dense(256))\n","  model.add(Dense(1, activation='sigmoid'))\n","\n","  model.summary()\n","  return model\n","\n","discriminator = build_discriminator()\n","discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 512)               401920    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 533,505\n","Trainable params: 533,505\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"TbcKcKmA0q2S"},"source":["## **5) Connecting Neural Networks to build GAN**"]},{"cell_type":"code","metadata":{"id":"q0Ue3TEd0xLy","executionInfo":{"status":"ok","timestamp":1631738943132,"user_tz":240,"elapsed":239,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}}},"source":["GAN = Sequential()\n","discriminator.trainable = False\n","GAN.add(generator)\n","GAN.add(discriminator)\n","\n","GAN.compile(loss='binary_crossentropy', optimizer=adam)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WaNhBDwRwTG"},"source":["## **6) Outputting Images**\n"]},{"cell_type":"code","metadata":{"id":"HQEJ0WbjRppy","executionInfo":{"status":"ok","timestamp":1631738943677,"user_tz":240,"elapsed":97,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}}},"source":["#@title\n","## **7) Outputting Images**\n","import matplotlib.pyplot as plt\n","import glob\n","import imageio\n","import PIL\n","\n","save_name = 0.00000000\n","\n","def save_imgs(epoch):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, latent_dim))\n","    gen_imgs = generator.predict(noise)\n","    global save_name\n","    save_name += 0.00000001\n","    # print(\"%.8f\" % save_name)\n","\n","    # Rescale images 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n","    plt.close()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tE57Lk5V0xs2"},"source":["## **7) Training GAN**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"egSJJvik00Iq","executionInfo":{"status":"error","timestamp":1631739042469,"user_tz":240,"elapsed":98120,"user":{"displayName":"Sam Boylan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1ogt32iqCUSQFFxx93gla3GPZ-DVLmvD0hGPtkw=s64","userId":"14155196384552992759"}},"outputId":"93c4ad8d-7eef-48b8-a759-5ad2b6608b6c"},"source":["def train(epochs, batch_size=64, save_interval=200):\n","  (X_train, _), (_, _) = mnist.load_data()\n","\n","  # print(X_train.shape)\n","  #Rescale data between -1 and 1\n","  X_train = X_train / 127.5 -1.\n","  X_train = np.expand_dims(X_train, axis=3)\n","  # print(X_train.shape)\n","\n","  #Create our Y for our Neural Networks\n","  valid = np.ones((batch_size, 1))\n","  fakes = np.zeros((batch_size, 1))\n","\n","  for epoch in range(epochs):\n","    #Get Random Batch\n","    idx = np.random.randint(0, X_train.shape[0], batch_size)\n","    imgs = X_train[idx]\n","\n","    #Generate Fake Images\n","    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","    gen_imgs = generator.predict(noise)\n","\n","    #Train discriminator\n","    d_loss_real = discriminator.train_on_batch(imgs, valid)\n","    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","    g_loss = GAN.train_on_batch(noise, valid)\n","\n","    print(\"******* %d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100* d_loss[1], g_loss))\n","\n","    if(epoch % save_interval) == 0:\n","      save_imgs(epoch)\n","\n","  # print(valid)\n","\n","\n","train(30000, batch_size=64, save_interval=200)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","******* 0 [D loss: 1.175835, acc: 9.38%] [G loss: 0.621972]\n","******* 1 [D loss: 0.545094, acc: 67.97%] [G loss: 0.599695]\n","******* 2 [D loss: 0.387179, acc: 72.66%] [G loss: 0.594323]\n","******* 3 [D loss: 0.383199, acc: 69.53%] [G loss: 0.637651]\n","******* 4 [D loss: 0.362436, acc: 75.78%] [G loss: 0.660782]\n","******* 5 [D loss: 0.355915, acc: 72.66%] [G loss: 0.641133]\n","******* 6 [D loss: 0.363250, acc: 77.34%] [G loss: 0.720921]\n","******* 7 [D loss: 0.363633, acc: 73.44%] [G loss: 0.776180]\n","******* 8 [D loss: 0.340324, acc: 79.69%] [G loss: 0.857246]\n","******* 9 [D loss: 0.325484, acc: 79.69%] [G loss: 0.952113]\n","******* 10 [D loss: 0.251850, acc: 92.19%] [G loss: 1.081968]\n","******* 11 [D loss: 0.232344, acc: 92.19%] [G loss: 1.207249]\n","******* 12 [D loss: 0.199041, acc: 98.44%] [G loss: 1.301517]\n","******* 13 [D loss: 0.175572, acc: 97.66%] [G loss: 1.377780]\n","******* 14 [D loss: 0.160999, acc: 99.22%] [G loss: 1.580093]\n","******* 15 [D loss: 0.134752, acc: 99.22%] [G loss: 1.696584]\n","******* 16 [D loss: 0.119583, acc: 100.00%] [G loss: 1.812279]\n","******* 17 [D loss: 0.103927, acc: 100.00%] [G loss: 1.950778]\n","******* 18 [D loss: 0.079392, acc: 100.00%] [G loss: 2.110959]\n","******* 19 [D loss: 0.072106, acc: 100.00%] [G loss: 2.333673]\n","******* 20 [D loss: 0.072275, acc: 100.00%] [G loss: 2.475563]\n","******* 21 [D loss: 0.058565, acc: 100.00%] [G loss: 2.496386]\n","******* 22 [D loss: 0.050684, acc: 100.00%] [G loss: 2.609796]\n","******* 23 [D loss: 0.047356, acc: 100.00%] [G loss: 2.638816]\n","******* 24 [D loss: 0.049519, acc: 100.00%] [G loss: 2.750618]\n","******* 25 [D loss: 0.035126, acc: 100.00%] [G loss: 2.848065]\n","******* 26 [D loss: 0.040654, acc: 100.00%] [G loss: 2.926642]\n","******* 27 [D loss: 0.037018, acc: 100.00%] [G loss: 2.830768]\n","******* 28 [D loss: 0.033953, acc: 100.00%] [G loss: 2.974465]\n","******* 29 [D loss: 0.038879, acc: 100.00%] [G loss: 3.031731]\n","******* 30 [D loss: 0.034789, acc: 100.00%] [G loss: 3.010304]\n","******* 31 [D loss: 0.036969, acc: 100.00%] [G loss: 3.056363]\n","******* 32 [D loss: 0.030588, acc: 100.00%] [G loss: 3.089249]\n","******* 33 [D loss: 0.031665, acc: 100.00%] [G loss: 3.102910]\n","******* 34 [D loss: 0.033126, acc: 100.00%] [G loss: 3.194368]\n","******* 35 [D loss: 0.028541, acc: 100.00%] [G loss: 3.217338]\n","******* 36 [D loss: 0.031978, acc: 100.00%] [G loss: 3.186786]\n","******* 37 [D loss: 0.029960, acc: 100.00%] [G loss: 3.133568]\n","******* 38 [D loss: 0.026975, acc: 100.00%] [G loss: 3.209512]\n","******* 39 [D loss: 0.031077, acc: 100.00%] [G loss: 3.232173]\n","******* 40 [D loss: 0.029723, acc: 100.00%] [G loss: 3.169000]\n","******* 41 [D loss: 0.022234, acc: 100.00%] [G loss: 3.281293]\n","******* 42 [D loss: 0.027190, acc: 100.00%] [G loss: 3.305695]\n","******* 43 [D loss: 0.030186, acc: 100.00%] [G loss: 3.321177]\n","******* 44 [D loss: 0.028570, acc: 100.00%] [G loss: 3.340121]\n","******* 45 [D loss: 0.024332, acc: 100.00%] [G loss: 3.291660]\n","******* 46 [D loss: 0.023876, acc: 100.00%] [G loss: 3.306530]\n","******* 47 [D loss: 0.018923, acc: 100.00%] [G loss: 3.352228]\n","******* 48 [D loss: 0.028604, acc: 100.00%] [G loss: 3.320407]\n","******* 49 [D loss: 0.026596, acc: 100.00%] [G loss: 3.281561]\n","******* 50 [D loss: 0.021617, acc: 100.00%] [G loss: 3.400774]\n","******* 51 [D loss: 0.024481, acc: 100.00%] [G loss: 3.403541]\n","******* 52 [D loss: 0.024581, acc: 100.00%] [G loss: 3.435373]\n","******* 53 [D loss: 0.027822, acc: 100.00%] [G loss: 3.432815]\n","******* 54 [D loss: 0.026597, acc: 100.00%] [G loss: 3.382752]\n","******* 55 [D loss: 0.022811, acc: 100.00%] [G loss: 3.407543]\n","******* 56 [D loss: 0.021319, acc: 100.00%] [G loss: 3.442274]\n","******* 57 [D loss: 0.023179, acc: 100.00%] [G loss: 3.409894]\n","******* 58 [D loss: 0.020098, acc: 100.00%] [G loss: 3.436348]\n","******* 59 [D loss: 0.020199, acc: 100.00%] [G loss: 3.533733]\n","******* 60 [D loss: 0.026545, acc: 100.00%] [G loss: 3.547650]\n","******* 61 [D loss: 0.020556, acc: 100.00%] [G loss: 3.548348]\n","******* 62 [D loss: 0.021601, acc: 100.00%] [G loss: 3.498053]\n","******* 63 [D loss: 0.021269, acc: 100.00%] [G loss: 3.674367]\n","******* 64 [D loss: 0.023776, acc: 100.00%] [G loss: 3.519006]\n","******* 65 [D loss: 0.019477, acc: 100.00%] [G loss: 3.585650]\n","******* 66 [D loss: 0.018492, acc: 100.00%] [G loss: 3.670634]\n","******* 67 [D loss: 0.020455, acc: 100.00%] [G loss: 3.617929]\n","******* 68 [D loss: 0.022933, acc: 100.00%] [G loss: 3.685552]\n","******* 69 [D loss: 0.018448, acc: 100.00%] [G loss: 3.775813]\n","******* 70 [D loss: 0.016963, acc: 100.00%] [G loss: 3.592518]\n","******* 71 [D loss: 0.017646, acc: 100.00%] [G loss: 3.669064]\n","******* 72 [D loss: 0.016076, acc: 100.00%] [G loss: 3.788874]\n","******* 73 [D loss: 0.016285, acc: 100.00%] [G loss: 3.811088]\n","******* 74 [D loss: 0.015790, acc: 100.00%] [G loss: 3.747497]\n","******* 75 [D loss: 0.016476, acc: 100.00%] [G loss: 3.843557]\n","******* 76 [D loss: 0.021453, acc: 100.00%] [G loss: 3.683920]\n","******* 77 [D loss: 0.014904, acc: 100.00%] [G loss: 3.775048]\n","******* 78 [D loss: 0.019887, acc: 100.00%] [G loss: 3.771776]\n","******* 79 [D loss: 0.019796, acc: 100.00%] [G loss: 3.884347]\n","******* 80 [D loss: 0.016655, acc: 100.00%] [G loss: 3.888797]\n","******* 81 [D loss: 0.016280, acc: 100.00%] [G loss: 3.892066]\n","******* 82 [D loss: 0.017005, acc: 100.00%] [G loss: 3.963290]\n","******* 83 [D loss: 0.018058, acc: 100.00%] [G loss: 3.820714]\n","******* 84 [D loss: 0.016858, acc: 100.00%] [G loss: 3.926382]\n","******* 85 [D loss: 0.015762, acc: 100.00%] [G loss: 4.007431]\n","******* 86 [D loss: 0.014220, acc: 100.00%] [G loss: 3.901855]\n","******* 87 [D loss: 0.015984, acc: 100.00%] [G loss: 3.986004]\n","******* 88 [D loss: 0.017023, acc: 100.00%] [G loss: 3.943836]\n","******* 89 [D loss: 0.014053, acc: 100.00%] [G loss: 4.015997]\n","******* 90 [D loss: 0.018504, acc: 100.00%] [G loss: 3.899383]\n","******* 91 [D loss: 0.014525, acc: 100.00%] [G loss: 4.089210]\n","******* 92 [D loss: 0.014230, acc: 100.00%] [G loss: 4.098679]\n","******* 93 [D loss: 0.011628, acc: 100.00%] [G loss: 4.056757]\n","******* 94 [D loss: 0.015654, acc: 100.00%] [G loss: 4.118285]\n","******* 95 [D loss: 0.014048, acc: 100.00%] [G loss: 4.252957]\n","******* 96 [D loss: 0.015683, acc: 100.00%] [G loss: 4.127645]\n","******* 97 [D loss: 0.014083, acc: 100.00%] [G loss: 4.009735]\n","******* 98 [D loss: 0.011569, acc: 100.00%] [G loss: 4.040025]\n","******* 99 [D loss: 0.012050, acc: 100.00%] [G loss: 4.125212]\n","******* 100 [D loss: 0.012141, acc: 100.00%] [G loss: 4.229065]\n","******* 101 [D loss: 0.011361, acc: 100.00%] [G loss: 4.111960]\n","******* 102 [D loss: 0.012493, acc: 100.00%] [G loss: 4.123807]\n","******* 103 [D loss: 0.014603, acc: 100.00%] [G loss: 4.165548]\n","******* 104 [D loss: 0.014511, acc: 100.00%] [G loss: 4.112781]\n","******* 105 [D loss: 0.016125, acc: 100.00%] [G loss: 4.195356]\n","******* 106 [D loss: 0.012936, acc: 100.00%] [G loss: 4.173948]\n","******* 107 [D loss: 0.011626, acc: 100.00%] [G loss: 4.134416]\n","******* 108 [D loss: 0.012967, acc: 100.00%] [G loss: 4.248623]\n","******* 109 [D loss: 0.013806, acc: 100.00%] [G loss: 4.172037]\n","******* 110 [D loss: 0.015226, acc: 100.00%] [G loss: 4.286566]\n","******* 111 [D loss: 0.014535, acc: 100.00%] [G loss: 4.249671]\n","******* 112 [D loss: 0.013644, acc: 100.00%] [G loss: 4.149067]\n","******* 113 [D loss: 0.013751, acc: 100.00%] [G loss: 4.197970]\n","******* 114 [D loss: 0.012246, acc: 100.00%] [G loss: 4.270118]\n","******* 115 [D loss: 0.010542, acc: 100.00%] [G loss: 4.182640]\n","******* 116 [D loss: 0.011752, acc: 100.00%] [G loss: 4.292776]\n","******* 117 [D loss: 0.014998, acc: 100.00%] [G loss: 4.203835]\n","******* 118 [D loss: 0.019257, acc: 100.00%] [G loss: 4.247280]\n","******* 119 [D loss: 0.015009, acc: 100.00%] [G loss: 4.218680]\n","******* 120 [D loss: 0.010787, acc: 100.00%] [G loss: 4.187543]\n","******* 121 [D loss: 0.013120, acc: 100.00%] [G loss: 4.310729]\n","******* 122 [D loss: 0.018019, acc: 100.00%] [G loss: 4.249382]\n","******* 123 [D loss: 0.015380, acc: 100.00%] [G loss: 4.171431]\n","******* 124 [D loss: 0.014166, acc: 100.00%] [G loss: 4.227991]\n","******* 125 [D loss: 0.015232, acc: 100.00%] [G loss: 4.379547]\n","******* 126 [D loss: 0.010593, acc: 100.00%] [G loss: 4.371099]\n","******* 127 [D loss: 0.015507, acc: 100.00%] [G loss: 4.310415]\n","******* 128 [D loss: 0.010363, acc: 100.00%] [G loss: 4.465591]\n","******* 129 [D loss: 0.012873, acc: 100.00%] [G loss: 4.455554]\n","******* 130 [D loss: 0.012326, acc: 100.00%] [G loss: 4.373833]\n","******* 131 [D loss: 0.014938, acc: 100.00%] [G loss: 4.279381]\n","******* 132 [D loss: 0.013745, acc: 100.00%] [G loss: 4.371537]\n","******* 133 [D loss: 0.013854, acc: 100.00%] [G loss: 4.349902]\n","******* 134 [D loss: 0.014694, acc: 100.00%] [G loss: 4.379069]\n","******* 135 [D loss: 0.012076, acc: 100.00%] [G loss: 4.415228]\n","******* 136 [D loss: 0.012697, acc: 100.00%] [G loss: 4.330096]\n","******* 137 [D loss: 0.008349, acc: 100.00%] [G loss: 4.381285]\n","******* 138 [D loss: 0.009621, acc: 100.00%] [G loss: 4.414804]\n","******* 139 [D loss: 0.015222, acc: 100.00%] [G loss: 4.337519]\n","******* 140 [D loss: 0.011036, acc: 100.00%] [G loss: 4.433303]\n","******* 141 [D loss: 0.011940, acc: 100.00%] [G loss: 4.387048]\n","******* 142 [D loss: 0.010757, acc: 100.00%] [G loss: 4.399638]\n","******* 143 [D loss: 0.017784, acc: 100.00%] [G loss: 4.373367]\n","******* 144 [D loss: 0.011545, acc: 100.00%] [G loss: 4.328979]\n","******* 145 [D loss: 0.014181, acc: 100.00%] [G loss: 4.417668]\n","******* 146 [D loss: 0.013538, acc: 100.00%] [G loss: 4.298061]\n","******* 147 [D loss: 0.013667, acc: 100.00%] [G loss: 4.371998]\n","******* 148 [D loss: 0.014101, acc: 100.00%] [G loss: 4.376511]\n","******* 149 [D loss: 0.015527, acc: 100.00%] [G loss: 4.259507]\n","******* 150 [D loss: 0.017619, acc: 100.00%] [G loss: 4.470187]\n","******* 151 [D loss: 0.011341, acc: 100.00%] [G loss: 4.556860]\n","******* 152 [D loss: 0.014711, acc: 100.00%] [G loss: 4.483251]\n","******* 153 [D loss: 0.013298, acc: 100.00%] [G loss: 4.311309]\n","******* 154 [D loss: 0.015738, acc: 100.00%] [G loss: 4.387239]\n","******* 155 [D loss: 0.013798, acc: 100.00%] [G loss: 4.467540]\n","******* 156 [D loss: 0.013155, acc: 100.00%] [G loss: 4.495563]\n","******* 157 [D loss: 0.014427, acc: 100.00%] [G loss: 4.533672]\n","******* 158 [D loss: 0.014459, acc: 100.00%] [G loss: 4.533292]\n","******* 159 [D loss: 0.013155, acc: 100.00%] [G loss: 4.427711]\n","******* 160 [D loss: 0.019373, acc: 100.00%] [G loss: 4.461891]\n","******* 161 [D loss: 0.013868, acc: 100.00%] [G loss: 4.563629]\n","******* 162 [D loss: 0.011266, acc: 100.00%] [G loss: 4.602471]\n","******* 163 [D loss: 0.013264, acc: 100.00%] [G loss: 4.655667]\n","******* 164 [D loss: 0.013782, acc: 100.00%] [G loss: 4.777987]\n","******* 165 [D loss: 0.012333, acc: 100.00%] [G loss: 4.665283]\n","******* 166 [D loss: 0.013061, acc: 100.00%] [G loss: 4.662347]\n","******* 167 [D loss: 0.013628, acc: 100.00%] [G loss: 4.592979]\n","******* 168 [D loss: 0.011235, acc: 100.00%] [G loss: 4.593348]\n","******* 169 [D loss: 0.011883, acc: 100.00%] [G loss: 4.585693]\n","******* 170 [D loss: 0.012829, acc: 100.00%] [G loss: 4.629087]\n","******* 171 [D loss: 0.016199, acc: 100.00%] [G loss: 4.554702]\n","******* 172 [D loss: 0.015629, acc: 100.00%] [G loss: 4.611578]\n","******* 173 [D loss: 0.016159, acc: 100.00%] [G loss: 4.566931]\n","******* 174 [D loss: 0.012775, acc: 100.00%] [G loss: 4.702518]\n","******* 175 [D loss: 0.012095, acc: 100.00%] [G loss: 4.803836]\n","******* 176 [D loss: 0.010560, acc: 100.00%] [G loss: 4.659844]\n","******* 177 [D loss: 0.013495, acc: 100.00%] [G loss: 4.630095]\n","******* 178 [D loss: 0.011378, acc: 100.00%] [G loss: 4.624437]\n","******* 179 [D loss: 0.011797, acc: 100.00%] [G loss: 4.577273]\n","******* 180 [D loss: 0.017213, acc: 100.00%] [G loss: 4.514340]\n","******* 181 [D loss: 0.011375, acc: 100.00%] [G loss: 4.449231]\n","******* 182 [D loss: 0.017788, acc: 100.00%] [G loss: 4.647404]\n","******* 183 [D loss: 0.016954, acc: 100.00%] [G loss: 4.691756]\n","******* 184 [D loss: 0.013528, acc: 100.00%] [G loss: 4.644154]\n","******* 185 [D loss: 0.013842, acc: 100.00%] [G loss: 4.690600]\n","******* 186 [D loss: 0.013538, acc: 100.00%] [G loss: 4.731406]\n","******* 187 [D loss: 0.018710, acc: 100.00%] [G loss: 4.766232]\n","******* 188 [D loss: 0.010197, acc: 100.00%] [G loss: 4.599387]\n","******* 189 [D loss: 0.014480, acc: 100.00%] [G loss: 4.803029]\n","******* 190 [D loss: 0.015362, acc: 100.00%] [G loss: 4.539980]\n","******* 191 [D loss: 0.011593, acc: 100.00%] [G loss: 4.633375]\n","******* 192 [D loss: 0.009918, acc: 100.00%] [G loss: 4.686108]\n","******* 193 [D loss: 0.014310, acc: 100.00%] [G loss: 4.653378]\n","******* 194 [D loss: 0.019855, acc: 100.00%] [G loss: 4.710266]\n","******* 195 [D loss: 0.015915, acc: 100.00%] [G loss: 4.686377]\n","******* 196 [D loss: 0.017496, acc: 100.00%] [G loss: 4.665288]\n","******* 197 [D loss: 0.015507, acc: 100.00%] [G loss: 4.861325]\n","******* 198 [D loss: 0.011080, acc: 100.00%] [G loss: 4.723515]\n","******* 199 [D loss: 0.013518, acc: 100.00%] [G loss: 4.886028]\n","******* 200 [D loss: 0.012432, acc: 100.00%] [G loss: 4.759528]\n","******* 201 [D loss: 0.017886, acc: 100.00%] [G loss: 4.803184]\n","******* 202 [D loss: 0.023828, acc: 100.00%] [G loss: 4.711912]\n","******* 203 [D loss: 0.019065, acc: 100.00%] [G loss: 4.692786]\n","******* 204 [D loss: 0.017190, acc: 100.00%] [G loss: 4.649044]\n","******* 205 [D loss: 0.016311, acc: 100.00%] [G loss: 4.693227]\n","******* 206 [D loss: 0.024131, acc: 100.00%] [G loss: 4.707085]\n","******* 207 [D loss: 0.014526, acc: 100.00%] [G loss: 4.817428]\n","******* 208 [D loss: 0.021010, acc: 100.00%] [G loss: 4.616271]\n","******* 209 [D loss: 0.013128, acc: 100.00%] [G loss: 4.749012]\n","******* 210 [D loss: 0.016521, acc: 100.00%] [G loss: 4.727772]\n","******* 211 [D loss: 0.013819, acc: 100.00%] [G loss: 4.803270]\n","******* 212 [D loss: 0.015523, acc: 100.00%] [G loss: 4.750879]\n","******* 213 [D loss: 0.013574, acc: 100.00%] [G loss: 4.659323]\n","******* 214 [D loss: 0.023824, acc: 100.00%] [G loss: 4.667277]\n","******* 215 [D loss: 0.022695, acc: 99.22%] [G loss: 4.726040]\n","******* 216 [D loss: 0.012164, acc: 100.00%] [G loss: 4.702865]\n","******* 217 [D loss: 0.017760, acc: 100.00%] [G loss: 4.524971]\n","******* 218 [D loss: 0.022615, acc: 100.00%] [G loss: 4.630784]\n","******* 219 [D loss: 0.019166, acc: 100.00%] [G loss: 4.538333]\n","******* 220 [D loss: 0.021304, acc: 100.00%] [G loss: 4.865373]\n","******* 221 [D loss: 0.022095, acc: 100.00%] [G loss: 4.784902]\n","******* 222 [D loss: 0.020878, acc: 100.00%] [G loss: 4.762068]\n","******* 223 [D loss: 0.017837, acc: 100.00%] [G loss: 4.829523]\n","******* 224 [D loss: 0.028531, acc: 100.00%] [G loss: 4.782666]\n","******* 225 [D loss: 0.019394, acc: 100.00%] [G loss: 4.640620]\n","******* 226 [D loss: 0.022080, acc: 100.00%] [G loss: 4.701745]\n","******* 227 [D loss: 0.029816, acc: 100.00%] [G loss: 4.637952]\n","******* 228 [D loss: 0.018723, acc: 100.00%] [G loss: 4.678349]\n","******* 229 [D loss: 0.016845, acc: 100.00%] [G loss: 4.417293]\n","******* 230 [D loss: 0.026006, acc: 100.00%] [G loss: 4.432334]\n","******* 231 [D loss: 0.029667, acc: 100.00%] [G loss: 4.576486]\n","******* 232 [D loss: 0.032976, acc: 99.22%] [G loss: 4.589688]\n","******* 233 [D loss: 0.018686, acc: 100.00%] [G loss: 4.996872]\n","******* 234 [D loss: 0.032306, acc: 100.00%] [G loss: 4.677598]\n","******* 235 [D loss: 0.025259, acc: 100.00%] [G loss: 4.594046]\n","******* 236 [D loss: 0.025761, acc: 100.00%] [G loss: 4.674186]\n","******* 237 [D loss: 0.016546, acc: 100.00%] [G loss: 4.426324]\n","******* 238 [D loss: 0.034177, acc: 100.00%] [G loss: 4.359381]\n","******* 239 [D loss: 0.020946, acc: 100.00%] [G loss: 4.557709]\n","******* 240 [D loss: 0.029959, acc: 100.00%] [G loss: 4.400334]\n","******* 241 [D loss: 0.021518, acc: 100.00%] [G loss: 4.676641]\n","******* 242 [D loss: 0.042087, acc: 99.22%] [G loss: 4.634702]\n","******* 243 [D loss: 0.026385, acc: 100.00%] [G loss: 4.751602]\n","******* 244 [D loss: 0.025168, acc: 100.00%] [G loss: 4.498168]\n","******* 245 [D loss: 0.026116, acc: 100.00%] [G loss: 4.460614]\n","******* 246 [D loss: 0.033594, acc: 100.00%] [G loss: 4.397658]\n","******* 247 [D loss: 0.038060, acc: 99.22%] [G loss: 4.515380]\n","******* 248 [D loss: 0.026205, acc: 100.00%] [G loss: 4.455745]\n","******* 249 [D loss: 0.036469, acc: 99.22%] [G loss: 4.454890]\n","******* 250 [D loss: 0.023301, acc: 100.00%] [G loss: 4.567193]\n","******* 251 [D loss: 0.037032, acc: 100.00%] [G loss: 4.307164]\n","******* 252 [D loss: 0.055528, acc: 98.44%] [G loss: 4.483277]\n","******* 253 [D loss: 0.044632, acc: 100.00%] [G loss: 4.668977]\n","******* 254 [D loss: 0.043231, acc: 99.22%] [G loss: 4.360474]\n","******* 255 [D loss: 0.034387, acc: 99.22%] [G loss: 4.161138]\n","******* 256 [D loss: 0.040729, acc: 99.22%] [G loss: 4.349519]\n","******* 257 [D loss: 0.045393, acc: 100.00%] [G loss: 4.419890]\n","******* 258 [D loss: 0.028261, acc: 100.00%] [G loss: 4.494034]\n","******* 259 [D loss: 0.032862, acc: 99.22%] [G loss: 4.271453]\n","******* 260 [D loss: 0.042047, acc: 100.00%] [G loss: 4.341972]\n","******* 261 [D loss: 0.036673, acc: 99.22%] [G loss: 4.403477]\n","******* 262 [D loss: 0.043411, acc: 98.44%] [G loss: 4.190231]\n","******* 263 [D loss: 0.047901, acc: 100.00%] [G loss: 4.061936]\n","******* 264 [D loss: 0.043734, acc: 99.22%] [G loss: 4.379086]\n","******* 265 [D loss: 0.052453, acc: 98.44%] [G loss: 4.342527]\n","******* 266 [D loss: 0.027015, acc: 100.00%] [G loss: 4.474820]\n","******* 267 [D loss: 0.033701, acc: 100.00%] [G loss: 4.419907]\n","******* 268 [D loss: 0.030500, acc: 100.00%] [G loss: 4.183844]\n","******* 269 [D loss: 0.059986, acc: 96.88%] [G loss: 4.148947]\n","******* 270 [D loss: 0.028690, acc: 100.00%] [G loss: 4.204544]\n","******* 271 [D loss: 0.047417, acc: 99.22%] [G loss: 4.076209]\n","******* 272 [D loss: 0.053231, acc: 99.22%] [G loss: 4.220873]\n","******* 273 [D loss: 0.060707, acc: 97.66%] [G loss: 4.484530]\n","******* 274 [D loss: 0.057362, acc: 100.00%] [G loss: 4.422439]\n","******* 275 [D loss: 0.048036, acc: 100.00%] [G loss: 4.535180]\n","******* 276 [D loss: 0.034940, acc: 100.00%] [G loss: 4.300585]\n","******* 277 [D loss: 0.044150, acc: 99.22%] [G loss: 4.018103]\n","******* 278 [D loss: 0.073144, acc: 97.66%] [G loss: 3.884076]\n","******* 279 [D loss: 0.075049, acc: 97.66%] [G loss: 3.984361]\n","******* 280 [D loss: 0.041958, acc: 100.00%] [G loss: 4.234322]\n","******* 281 [D loss: 0.047549, acc: 99.22%] [G loss: 4.420781]\n","******* 282 [D loss: 0.101586, acc: 97.66%] [G loss: 4.052442]\n","******* 283 [D loss: 0.047175, acc: 99.22%] [G loss: 4.001890]\n","******* 284 [D loss: 0.060809, acc: 97.66%] [G loss: 3.935037]\n","******* 285 [D loss: 0.038312, acc: 99.22%] [G loss: 4.098438]\n","******* 286 [D loss: 0.053601, acc: 99.22%] [G loss: 4.005500]\n","******* 287 [D loss: 0.063776, acc: 97.66%] [G loss: 4.286709]\n","******* 288 [D loss: 0.066637, acc: 97.66%] [G loss: 4.327719]\n","******* 289 [D loss: 0.072600, acc: 98.44%] [G loss: 4.194810]\n","******* 290 [D loss: 0.085275, acc: 97.66%] [G loss: 4.382613]\n","******* 291 [D loss: 0.044233, acc: 98.44%] [G loss: 4.155242]\n","******* 292 [D loss: 0.060226, acc: 97.66%] [G loss: 3.873627]\n","******* 293 [D loss: 0.075602, acc: 97.66%] [G loss: 3.677385]\n","******* 294 [D loss: 0.047081, acc: 98.44%] [G loss: 3.876389]\n","******* 295 [D loss: 0.050646, acc: 100.00%] [G loss: 4.242260]\n","******* 296 [D loss: 0.067864, acc: 99.22%] [G loss: 4.063354]\n","******* 297 [D loss: 0.062501, acc: 98.44%] [G loss: 3.845845]\n","******* 298 [D loss: 0.068226, acc: 98.44%] [G loss: 3.609769]\n","******* 299 [D loss: 0.046303, acc: 100.00%] [G loss: 3.698143]\n","******* 300 [D loss: 0.091303, acc: 95.31%] [G loss: 3.452206]\n","******* 301 [D loss: 0.059171, acc: 99.22%] [G loss: 3.849503]\n","******* 302 [D loss: 0.048828, acc: 99.22%] [G loss: 3.938378]\n","******* 303 [D loss: 0.068376, acc: 99.22%] [G loss: 3.835281]\n","******* 304 [D loss: 0.096450, acc: 97.66%] [G loss: 3.423159]\n","******* 305 [D loss: 0.106673, acc: 94.53%] [G loss: 3.401510]\n","******* 306 [D loss: 0.080376, acc: 98.44%] [G loss: 3.647862]\n","******* 307 [D loss: 0.060248, acc: 96.88%] [G loss: 3.930011]\n","******* 308 [D loss: 0.091863, acc: 98.44%] [G loss: 3.961149]\n","******* 309 [D loss: 0.107318, acc: 96.09%] [G loss: 3.985793]\n","******* 310 [D loss: 0.105745, acc: 96.88%] [G loss: 3.704603]\n","******* 311 [D loss: 0.065182, acc: 99.22%] [G loss: 3.911209]\n","******* 312 [D loss: 0.065903, acc: 99.22%] [G loss: 4.026584]\n","******* 313 [D loss: 0.066506, acc: 99.22%] [G loss: 4.034313]\n","******* 314 [D loss: 0.080280, acc: 99.22%] [G loss: 3.633916]\n","******* 315 [D loss: 0.066815, acc: 98.44%] [G loss: 3.654080]\n","******* 316 [D loss: 0.085650, acc: 96.88%] [G loss: 3.673710]\n","******* 317 [D loss: 0.074561, acc: 97.66%] [G loss: 3.823767]\n","******* 318 [D loss: 0.066167, acc: 99.22%] [G loss: 3.991957]\n","******* 319 [D loss: 0.057252, acc: 98.44%] [G loss: 3.543658]\n","******* 320 [D loss: 0.117128, acc: 94.53%] [G loss: 3.741852]\n","******* 321 [D loss: 0.088279, acc: 96.88%] [G loss: 3.742590]\n","******* 322 [D loss: 0.094094, acc: 97.66%] [G loss: 3.925096]\n","******* 323 [D loss: 0.071228, acc: 96.88%] [G loss: 4.177555]\n","******* 324 [D loss: 0.070345, acc: 97.66%] [G loss: 4.078875]\n","******* 325 [D loss: 0.058518, acc: 100.00%] [G loss: 3.769325]\n","******* 326 [D loss: 0.070433, acc: 97.66%] [G loss: 4.082277]\n","******* 327 [D loss: 0.074314, acc: 98.44%] [G loss: 3.841275]\n","******* 328 [D loss: 0.054230, acc: 100.00%] [G loss: 4.160681]\n","******* 329 [D loss: 0.074289, acc: 99.22%] [G loss: 3.970019]\n","******* 330 [D loss: 0.076367, acc: 96.09%] [G loss: 3.808732]\n","******* 331 [D loss: 0.089836, acc: 96.88%] [G loss: 4.090576]\n","******* 332 [D loss: 0.068448, acc: 97.66%] [G loss: 4.142063]\n","******* 333 [D loss: 0.078893, acc: 96.88%] [G loss: 4.120453]\n","******* 334 [D loss: 0.070456, acc: 97.66%] [G loss: 4.043373]\n","******* 335 [D loss: 0.064000, acc: 99.22%] [G loss: 4.130657]\n","******* 336 [D loss: 0.062396, acc: 99.22%] [G loss: 4.266123]\n","******* 337 [D loss: 0.050575, acc: 99.22%] [G loss: 4.325217]\n","******* 338 [D loss: 0.079962, acc: 100.00%] [G loss: 4.085274]\n","******* 339 [D loss: 0.077777, acc: 99.22%] [G loss: 3.897485]\n","******* 340 [D loss: 0.066027, acc: 96.88%] [G loss: 3.790012]\n","******* 341 [D loss: 0.091714, acc: 98.44%] [G loss: 3.936986]\n","******* 342 [D loss: 0.085202, acc: 97.66%] [G loss: 4.081058]\n","******* 343 [D loss: 0.058153, acc: 99.22%] [G loss: 4.492749]\n","******* 344 [D loss: 0.080382, acc: 98.44%] [G loss: 4.313330]\n","******* 345 [D loss: 0.061792, acc: 97.66%] [G loss: 4.360848]\n","******* 346 [D loss: 0.083625, acc: 99.22%] [G loss: 4.123159]\n","******* 347 [D loss: 0.064769, acc: 98.44%] [G loss: 4.057905]\n","******* 348 [D loss: 0.071886, acc: 98.44%] [G loss: 4.079124]\n","******* 349 [D loss: 0.065601, acc: 99.22%] [G loss: 4.134895]\n","******* 350 [D loss: 0.074316, acc: 97.66%] [G loss: 4.232618]\n","******* 351 [D loss: 0.051406, acc: 99.22%] [G loss: 4.143480]\n","******* 352 [D loss: 0.058894, acc: 99.22%] [G loss: 4.341669]\n","******* 353 [D loss: 0.092253, acc: 97.66%] [G loss: 4.374245]\n","******* 354 [D loss: 0.048254, acc: 99.22%] [G loss: 4.234344]\n","******* 355 [D loss: 0.096570, acc: 96.88%] [G loss: 3.659909]\n","******* 356 [D loss: 0.129835, acc: 94.53%] [G loss: 3.868188]\n","******* 357 [D loss: 0.066328, acc: 97.66%] [G loss: 4.431598]\n","******* 358 [D loss: 0.057532, acc: 98.44%] [G loss: 4.513336]\n","******* 359 [D loss: 0.060598, acc: 99.22%] [G loss: 4.666793]\n","******* 360 [D loss: 0.069745, acc: 97.66%] [G loss: 4.046926]\n","******* 361 [D loss: 0.075015, acc: 98.44%] [G loss: 3.967147]\n","******* 362 [D loss: 0.049491, acc: 99.22%] [G loss: 3.855334]\n","******* 363 [D loss: 0.037727, acc: 100.00%] [G loss: 4.397942]\n","******* 364 [D loss: 0.054121, acc: 99.22%] [G loss: 3.898210]\n","******* 365 [D loss: 0.051614, acc: 99.22%] [G loss: 4.231125]\n","******* 366 [D loss: 0.071051, acc: 99.22%] [G loss: 4.204418]\n","******* 367 [D loss: 0.095412, acc: 96.09%] [G loss: 4.150698]\n","******* 368 [D loss: 0.076715, acc: 98.44%] [G loss: 4.151299]\n","******* 369 [D loss: 0.062234, acc: 98.44%] [G loss: 4.386027]\n","******* 370 [D loss: 0.072172, acc: 100.00%] [G loss: 4.378798]\n","******* 371 [D loss: 0.045447, acc: 100.00%] [G loss: 4.713744]\n","******* 372 [D loss: 0.097828, acc: 97.66%] [G loss: 4.193039]\n","******* 373 [D loss: 0.095138, acc: 96.88%] [G loss: 3.944004]\n","******* 374 [D loss: 0.047501, acc: 100.00%] [G loss: 4.488080]\n","******* 375 [D loss: 0.047816, acc: 99.22%] [G loss: 4.894981]\n","******* 376 [D loss: 0.036415, acc: 100.00%] [G loss: 4.737423]\n","******* 377 [D loss: 0.038450, acc: 99.22%] [G loss: 4.903399]\n","******* 378 [D loss: 0.041261, acc: 100.00%] [G loss: 4.785342]\n","******* 379 [D loss: 0.062092, acc: 99.22%] [G loss: 4.659863]\n","******* 380 [D loss: 0.049558, acc: 99.22%] [G loss: 4.135967]\n","******* 381 [D loss: 0.051190, acc: 99.22%] [G loss: 4.089335]\n","******* 382 [D loss: 0.053903, acc: 98.44%] [G loss: 4.298235]\n","******* 383 [D loss: 0.039344, acc: 98.44%] [G loss: 4.829772]\n","******* 384 [D loss: 0.053590, acc: 99.22%] [G loss: 4.699962]\n","******* 385 [D loss: 0.041527, acc: 100.00%] [G loss: 4.399786]\n","******* 386 [D loss: 0.045214, acc: 100.00%] [G loss: 4.268557]\n","******* 387 [D loss: 0.053064, acc: 99.22%] [G loss: 4.635170]\n","******* 388 [D loss: 0.047702, acc: 99.22%] [G loss: 4.841439]\n","******* 389 [D loss: 0.048070, acc: 100.00%] [G loss: 5.455939]\n","******* 390 [D loss: 0.033673, acc: 100.00%] [G loss: 5.070513]\n","******* 391 [D loss: 0.046327, acc: 100.00%] [G loss: 4.718127]\n","******* 392 [D loss: 0.036408, acc: 100.00%] [G loss: 4.151808]\n","******* 393 [D loss: 0.068683, acc: 96.88%] [G loss: 4.295014]\n","******* 394 [D loss: 0.042489, acc: 99.22%] [G loss: 4.156104]\n","******* 395 [D loss: 0.086511, acc: 97.66%] [G loss: 4.234869]\n","******* 396 [D loss: 0.043304, acc: 100.00%] [G loss: 4.069344]\n","******* 397 [D loss: 0.023680, acc: 100.00%] [G loss: 4.815191]\n","******* 398 [D loss: 0.043870, acc: 99.22%] [G loss: 4.966115]\n","******* 399 [D loss: 0.052706, acc: 99.22%] [G loss: 4.826497]\n","******* 400 [D loss: 0.049768, acc: 99.22%] [G loss: 4.602221]\n","******* 401 [D loss: 0.043178, acc: 99.22%] [G loss: 4.597700]\n","******* 402 [D loss: 0.034760, acc: 100.00%] [G loss: 4.563734]\n","******* 403 [D loss: 0.037500, acc: 100.00%] [G loss: 4.319152]\n","******* 404 [D loss: 0.032345, acc: 99.22%] [G loss: 4.625072]\n","******* 405 [D loss: 0.030158, acc: 100.00%] [G loss: 4.831146]\n","******* 406 [D loss: 0.032972, acc: 100.00%] [G loss: 4.999886]\n","******* 407 [D loss: 0.038165, acc: 99.22%] [G loss: 4.545868]\n","******* 408 [D loss: 0.028741, acc: 100.00%] [G loss: 4.363710]\n","******* 409 [D loss: 0.035104, acc: 100.00%] [G loss: 4.116911]\n","******* 410 [D loss: 0.052045, acc: 99.22%] [G loss: 4.799207]\n","******* 411 [D loss: 0.035208, acc: 100.00%] [G loss: 4.951967]\n","******* 412 [D loss: 0.047159, acc: 99.22%] [G loss: 4.886336]\n","******* 413 [D loss: 0.034093, acc: 100.00%] [G loss: 4.812234]\n","******* 414 [D loss: 0.037152, acc: 100.00%] [G loss: 4.695800]\n","******* 415 [D loss: 0.030870, acc: 100.00%] [G loss: 4.874430]\n","******* 416 [D loss: 0.040362, acc: 99.22%] [G loss: 4.801757]\n","******* 417 [D loss: 0.031801, acc: 100.00%] [G loss: 4.763786]\n","******* 418 [D loss: 0.040726, acc: 99.22%] [G loss: 5.033062]\n","******* 419 [D loss: 0.034196, acc: 99.22%] [G loss: 5.136889]\n","******* 420 [D loss: 0.037077, acc: 99.22%] [G loss: 4.783714]\n","******* 421 [D loss: 0.028659, acc: 100.00%] [G loss: 4.579895]\n","******* 422 [D loss: 0.028703, acc: 100.00%] [G loss: 4.770435]\n","******* 423 [D loss: 0.027196, acc: 99.22%] [G loss: 4.540513]\n","******* 424 [D loss: 0.027576, acc: 100.00%] [G loss: 4.711200]\n","******* 425 [D loss: 0.069963, acc: 97.66%] [G loss: 4.416159]\n","******* 426 [D loss: 0.035007, acc: 99.22%] [G loss: 4.606798]\n","******* 427 [D loss: 0.046467, acc: 100.00%] [G loss: 4.554173]\n","******* 428 [D loss: 0.030756, acc: 100.00%] [G loss: 5.234181]\n","******* 429 [D loss: 0.041563, acc: 99.22%] [G loss: 5.212726]\n","******* 430 [D loss: 0.033215, acc: 99.22%] [G loss: 4.708776]\n","******* 431 [D loss: 0.063216, acc: 97.66%] [G loss: 4.209231]\n","******* 432 [D loss: 0.045612, acc: 100.00%] [G loss: 4.241303]\n","******* 433 [D loss: 0.038474, acc: 100.00%] [G loss: 4.867651]\n","******* 434 [D loss: 0.019132, acc: 100.00%] [G loss: 5.532006]\n","******* 435 [D loss: 0.022338, acc: 99.22%] [G loss: 5.842154]\n","******* 436 [D loss: 0.035913, acc: 99.22%] [G loss: 5.489596]\n","******* 437 [D loss: 0.035418, acc: 99.22%] [G loss: 5.327426]\n","******* 438 [D loss: 0.020037, acc: 100.00%] [G loss: 5.304945]\n","******* 439 [D loss: 0.021976, acc: 100.00%] [G loss: 4.989640]\n","******* 440 [D loss: 0.026149, acc: 100.00%] [G loss: 4.767806]\n","******* 441 [D loss: 0.045446, acc: 99.22%] [G loss: 4.926150]\n","******* 442 [D loss: 0.037983, acc: 100.00%] [G loss: 5.032053]\n","******* 443 [D loss: 0.068198, acc: 99.22%] [G loss: 4.591045]\n","******* 444 [D loss: 0.034623, acc: 100.00%] [G loss: 5.008636]\n","******* 445 [D loss: 0.045877, acc: 100.00%] [G loss: 4.858068]\n","******* 446 [D loss: 0.032136, acc: 100.00%] [G loss: 4.578592]\n","******* 447 [D loss: 0.025880, acc: 100.00%] [G loss: 4.891663]\n","******* 448 [D loss: 0.020263, acc: 100.00%] [G loss: 5.274462]\n","******* 449 [D loss: 0.022894, acc: 100.00%] [G loss: 5.186749]\n","******* 450 [D loss: 0.045671, acc: 99.22%] [G loss: 4.745112]\n","******* 451 [D loss: 0.031794, acc: 100.00%] [G loss: 4.559764]\n","******* 452 [D loss: 0.033897, acc: 100.00%] [G loss: 4.377681]\n","******* 453 [D loss: 0.042587, acc: 98.44%] [G loss: 4.866972]\n","******* 454 [D loss: 0.041785, acc: 98.44%] [G loss: 5.120056]\n","******* 455 [D loss: 0.026260, acc: 100.00%] [G loss: 5.367712]\n","******* 456 [D loss: 0.032950, acc: 99.22%] [G loss: 5.291503]\n","******* 457 [D loss: 0.025857, acc: 100.00%] [G loss: 5.060824]\n","******* 458 [D loss: 0.042029, acc: 100.00%] [G loss: 4.819369]\n","******* 459 [D loss: 0.041123, acc: 100.00%] [G loss: 5.115506]\n","******* 460 [D loss: 0.031465, acc: 99.22%] [G loss: 5.008576]\n","******* 461 [D loss: 0.031050, acc: 100.00%] [G loss: 4.944270]\n","******* 462 [D loss: 0.029617, acc: 100.00%] [G loss: 4.947305]\n","******* 463 [D loss: 0.031238, acc: 100.00%] [G loss: 5.145314]\n","******* 464 [D loss: 0.040134, acc: 99.22%] [G loss: 5.197199]\n","******* 465 [D loss: 0.024636, acc: 100.00%] [G loss: 5.175371]\n","******* 466 [D loss: 0.024023, acc: 100.00%] [G loss: 5.476288]\n","******* 467 [D loss: 0.032605, acc: 99.22%] [G loss: 5.666523]\n","******* 468 [D loss: 0.030227, acc: 99.22%] [G loss: 5.584565]\n","******* 469 [D loss: 0.030023, acc: 100.00%] [G loss: 5.065177]\n","******* 470 [D loss: 0.045545, acc: 97.66%] [G loss: 4.985232]\n","******* 471 [D loss: 0.036834, acc: 100.00%] [G loss: 5.075477]\n","******* 472 [D loss: 0.028837, acc: 99.22%] [G loss: 5.400668]\n","******* 473 [D loss: 0.042457, acc: 99.22%] [G loss: 5.303067]\n","******* 474 [D loss: 0.047944, acc: 99.22%] [G loss: 4.814252]\n","******* 475 [D loss: 0.019927, acc: 100.00%] [G loss: 4.853783]\n","******* 476 [D loss: 0.045989, acc: 99.22%] [G loss: 4.815817]\n","******* 477 [D loss: 0.021132, acc: 100.00%] [G loss: 4.569607]\n","******* 478 [D loss: 0.031276, acc: 100.00%] [G loss: 4.688585]\n","******* 479 [D loss: 0.024917, acc: 99.22%] [G loss: 4.913846]\n","******* 480 [D loss: 0.039514, acc: 98.44%] [G loss: 4.984026]\n","******* 481 [D loss: 0.027704, acc: 100.00%] [G loss: 5.418927]\n","******* 482 [D loss: 0.036884, acc: 100.00%] [G loss: 5.120104]\n","******* 483 [D loss: 0.028486, acc: 100.00%] [G loss: 5.158350]\n","******* 484 [D loss: 0.062366, acc: 98.44%] [G loss: 4.911109]\n","******* 485 [D loss: 0.073825, acc: 97.66%] [G loss: 4.746456]\n","******* 486 [D loss: 0.039119, acc: 99.22%] [G loss: 5.666180]\n","******* 487 [D loss: 0.035152, acc: 99.22%] [G loss: 5.406113]\n","******* 488 [D loss: 0.032751, acc: 100.00%] [G loss: 5.277849]\n","******* 489 [D loss: 0.035457, acc: 99.22%] [G loss: 5.688563]\n","******* 490 [D loss: 0.042705, acc: 99.22%] [G loss: 5.154405]\n","******* 491 [D loss: 0.040109, acc: 100.00%] [G loss: 4.437749]\n","******* 492 [D loss: 0.067893, acc: 98.44%] [G loss: 4.674177]\n","******* 493 [D loss: 0.044110, acc: 98.44%] [G loss: 5.236312]\n","******* 494 [D loss: 0.029089, acc: 99.22%] [G loss: 5.341112]\n","******* 495 [D loss: 0.042961, acc: 99.22%] [G loss: 5.116367]\n","******* 496 [D loss: 0.036422, acc: 100.00%] [G loss: 4.581834]\n","******* 497 [D loss: 0.026251, acc: 100.00%] [G loss: 4.757552]\n","******* 498 [D loss: 0.046300, acc: 100.00%] [G loss: 4.587508]\n","******* 499 [D loss: 0.022978, acc: 100.00%] [G loss: 4.701728]\n","******* 500 [D loss: 0.057464, acc: 99.22%] [G loss: 4.757127]\n","******* 501 [D loss: 0.057997, acc: 98.44%] [G loss: 5.075401]\n","******* 502 [D loss: 0.064601, acc: 96.88%] [G loss: 5.319756]\n","******* 503 [D loss: 0.039370, acc: 98.44%] [G loss: 5.451842]\n","******* 504 [D loss: 0.051882, acc: 100.00%] [G loss: 5.095923]\n","******* 505 [D loss: 0.060275, acc: 96.09%] [G loss: 4.665686]\n","******* 506 [D loss: 0.077349, acc: 96.88%] [G loss: 5.150592]\n","******* 507 [D loss: 0.025246, acc: 100.00%] [G loss: 6.081256]\n","******* 508 [D loss: 0.062208, acc: 99.22%] [G loss: 5.709528]\n","******* 509 [D loss: 0.039761, acc: 99.22%] [G loss: 5.097354]\n","******* 510 [D loss: 0.039512, acc: 100.00%] [G loss: 5.469092]\n","******* 511 [D loss: 0.016964, acc: 100.00%] [G loss: 5.753931]\n","******* 512 [D loss: 0.060284, acc: 98.44%] [G loss: 5.506565]\n","******* 513 [D loss: 0.027731, acc: 99.22%] [G loss: 5.119924]\n","******* 514 [D loss: 0.045672, acc: 99.22%] [G loss: 4.804946]\n","******* 515 [D loss: 0.055283, acc: 98.44%] [G loss: 5.005678]\n","******* 516 [D loss: 0.044722, acc: 100.00%] [G loss: 4.780077]\n","******* 517 [D loss: 0.037063, acc: 100.00%] [G loss: 4.482588]\n","******* 518 [D loss: 0.029369, acc: 100.00%] [G loss: 4.544676]\n","******* 519 [D loss: 0.094693, acc: 95.31%] [G loss: 4.808684]\n","******* 520 [D loss: 0.078756, acc: 97.66%] [G loss: 4.604640]\n","******* 521 [D loss: 0.053414, acc: 99.22%] [G loss: 4.867983]\n","******* 522 [D loss: 0.038010, acc: 100.00%] [G loss: 5.033721]\n","******* 523 [D loss: 0.052086, acc: 99.22%] [G loss: 4.659543]\n","******* 524 [D loss: 0.052395, acc: 100.00%] [G loss: 4.646419]\n","******* 525 [D loss: 0.041499, acc: 100.00%] [G loss: 4.933383]\n","******* 526 [D loss: 0.040186, acc: 100.00%] [G loss: 5.485558]\n","******* 527 [D loss: 0.046482, acc: 100.00%] [G loss: 4.927321]\n","******* 528 [D loss: 0.055732, acc: 98.44%] [G loss: 4.579160]\n","******* 529 [D loss: 0.043365, acc: 99.22%] [G loss: 4.759603]\n","******* 530 [D loss: 0.030776, acc: 100.00%] [G loss: 5.237689]\n","******* 531 [D loss: 0.054333, acc: 99.22%] [G loss: 5.122072]\n","******* 532 [D loss: 0.054755, acc: 99.22%] [G loss: 4.803490]\n","******* 533 [D loss: 0.056079, acc: 100.00%] [G loss: 5.055588]\n","******* 534 [D loss: 0.034520, acc: 100.00%] [G loss: 5.545560]\n","******* 535 [D loss: 0.055192, acc: 99.22%] [G loss: 5.536778]\n","******* 536 [D loss: 0.043581, acc: 100.00%] [G loss: 4.877084]\n","******* 537 [D loss: 0.082735, acc: 98.44%] [G loss: 4.351919]\n","******* 538 [D loss: 0.051709, acc: 100.00%] [G loss: 4.937028]\n","******* 539 [D loss: 0.035786, acc: 99.22%] [G loss: 5.403162]\n","******* 540 [D loss: 0.032945, acc: 100.00%] [G loss: 5.356687]\n","******* 541 [D loss: 0.062055, acc: 98.44%] [G loss: 4.244006]\n","******* 542 [D loss: 0.079534, acc: 97.66%] [G loss: 4.279295]\n","******* 543 [D loss: 0.037714, acc: 99.22%] [G loss: 5.541332]\n","******* 544 [D loss: 0.056782, acc: 100.00%] [G loss: 5.751712]\n","******* 545 [D loss: 0.085408, acc: 97.66%] [G loss: 4.880132]\n","******* 546 [D loss: 0.093533, acc: 96.88%] [G loss: 4.845417]\n","******* 547 [D loss: 0.062014, acc: 98.44%] [G loss: 5.292173]\n","******* 548 [D loss: 0.035751, acc: 100.00%] [G loss: 5.620924]\n","******* 549 [D loss: 0.061151, acc: 98.44%] [G loss: 5.505308]\n","******* 550 [D loss: 0.058142, acc: 99.22%] [G loss: 5.010921]\n","******* 551 [D loss: 0.081971, acc: 97.66%] [G loss: 4.662179]\n","******* 552 [D loss: 0.101272, acc: 95.31%] [G loss: 4.761126]\n","******* 553 [D loss: 0.018966, acc: 100.00%] [G loss: 5.871285]\n","******* 554 [D loss: 0.061050, acc: 98.44%] [G loss: 5.621930]\n","******* 555 [D loss: 0.055820, acc: 99.22%] [G loss: 4.331221]\n","******* 556 [D loss: 0.074847, acc: 99.22%] [G loss: 4.067612]\n","******* 557 [D loss: 0.041926, acc: 99.22%] [G loss: 4.735214]\n","******* 558 [D loss: 0.023377, acc: 100.00%] [G loss: 5.030448]\n","******* 559 [D loss: 0.068148, acc: 99.22%] [G loss: 5.234348]\n","******* 560 [D loss: 0.050459, acc: 99.22%] [G loss: 4.447470]\n","******* 561 [D loss: 0.069690, acc: 99.22%] [G loss: 4.606756]\n","******* 562 [D loss: 0.085732, acc: 96.88%] [G loss: 5.113482]\n","******* 563 [D loss: 0.032304, acc: 99.22%] [G loss: 5.452623]\n","******* 564 [D loss: 0.059876, acc: 98.44%] [G loss: 5.213027]\n","******* 565 [D loss: 0.103337, acc: 95.31%] [G loss: 4.673384]\n","******* 566 [D loss: 0.101148, acc: 97.66%] [G loss: 4.911645]\n","******* 567 [D loss: 0.039870, acc: 98.44%] [G loss: 6.431361]\n","******* 568 [D loss: 0.070985, acc: 97.66%] [G loss: 5.953865]\n","******* 569 [D loss: 0.120572, acc: 96.09%] [G loss: 5.035161]\n","******* 570 [D loss: 0.120842, acc: 96.09%] [G loss: 5.039661]\n","******* 571 [D loss: 0.050621, acc: 98.44%] [G loss: 6.520843]\n","******* 572 [D loss: 0.039399, acc: 99.22%] [G loss: 6.955225]\n","******* 573 [D loss: 0.041922, acc: 100.00%] [G loss: 6.226577]\n","******* 574 [D loss: 0.083900, acc: 95.31%] [G loss: 4.789985]\n","******* 575 [D loss: 0.081736, acc: 97.66%] [G loss: 4.212163]\n","******* 576 [D loss: 0.059734, acc: 96.88%] [G loss: 5.151006]\n","******* 577 [D loss: 0.023614, acc: 100.00%] [G loss: 5.715669]\n","******* 578 [D loss: 0.064041, acc: 98.44%] [G loss: 5.990878]\n","******* 579 [D loss: 0.042976, acc: 98.44%] [G loss: 5.127846]\n","******* 580 [D loss: 0.055780, acc: 99.22%] [G loss: 4.343779]\n","******* 581 [D loss: 0.039974, acc: 100.00%] [G loss: 4.751573]\n","******* 582 [D loss: 0.020667, acc: 100.00%] [G loss: 5.564223]\n","******* 583 [D loss: 0.033851, acc: 99.22%] [G loss: 5.568376]\n","******* 584 [D loss: 0.083448, acc: 98.44%] [G loss: 4.414105]\n","******* 585 [D loss: 0.076839, acc: 97.66%] [G loss: 4.230477]\n","******* 586 [D loss: 0.033917, acc: 99.22%] [G loss: 5.262114]\n","******* 587 [D loss: 0.029684, acc: 100.00%] [G loss: 5.756354]\n","******* 588 [D loss: 0.083837, acc: 98.44%] [G loss: 5.491818]\n","******* 589 [D loss: 0.059874, acc: 99.22%] [G loss: 4.902710]\n","******* 590 [D loss: 0.108107, acc: 96.88%] [G loss: 4.617218]\n","******* 591 [D loss: 0.094359, acc: 96.88%] [G loss: 5.540106]\n","******* 592 [D loss: 0.035431, acc: 100.00%] [G loss: 6.012148]\n","******* 593 [D loss: 0.057498, acc: 98.44%] [G loss: 5.389538]\n","******* 594 [D loss: 0.102616, acc: 96.09%] [G loss: 4.339646]\n","******* 595 [D loss: 0.038230, acc: 100.00%] [G loss: 4.536141]\n","******* 596 [D loss: 0.081964, acc: 96.09%] [G loss: 6.117621]\n","******* 597 [D loss: 0.118456, acc: 94.53%] [G loss: 5.820620]\n","******* 598 [D loss: 0.023036, acc: 100.00%] [G loss: 5.820518]\n","******* 599 [D loss: 0.038389, acc: 99.22%] [G loss: 5.602694]\n","******* 600 [D loss: 0.017649, acc: 100.00%] [G loss: 5.482942]\n","******* 601 [D loss: 0.050418, acc: 100.00%] [G loss: 4.865506]\n","******* 602 [D loss: 0.033478, acc: 100.00%] [G loss: 5.415933]\n","******* 603 [D loss: 0.076724, acc: 99.22%] [G loss: 5.283299]\n","******* 604 [D loss: 0.045849, acc: 99.22%] [G loss: 5.359874]\n","******* 605 [D loss: 0.029782, acc: 100.00%] [G loss: 5.648127]\n","******* 606 [D loss: 0.096306, acc: 96.88%] [G loss: 4.510187]\n","******* 607 [D loss: 0.080983, acc: 97.66%] [G loss: 3.922245]\n","******* 608 [D loss: 0.085124, acc: 98.44%] [G loss: 4.672535]\n","******* 609 [D loss: 0.058993, acc: 98.44%] [G loss: 5.423383]\n","******* 610 [D loss: 0.168474, acc: 93.75%] [G loss: 4.299224]\n","******* 611 [D loss: 0.173012, acc: 91.41%] [G loss: 4.509610]\n","******* 612 [D loss: 0.030474, acc: 100.00%] [G loss: 7.008881]\n","******* 613 [D loss: 0.102640, acc: 96.88%] [G loss: 6.134521]\n","******* 614 [D loss: 0.100040, acc: 96.09%] [G loss: 4.881512]\n","******* 615 [D loss: 0.030132, acc: 100.00%] [G loss: 4.567546]\n","******* 616 [D loss: 0.069257, acc: 96.88%] [G loss: 5.091483]\n","******* 617 [D loss: 0.033940, acc: 100.00%] [G loss: 6.365099]\n","******* 618 [D loss: 0.042745, acc: 99.22%] [G loss: 6.609559]\n","******* 619 [D loss: 0.081635, acc: 99.22%] [G loss: 5.138424]\n","******* 620 [D loss: 0.042342, acc: 100.00%] [G loss: 4.362213]\n","******* 621 [D loss: 0.068366, acc: 98.44%] [G loss: 4.739596]\n","******* 622 [D loss: 0.041074, acc: 98.44%] [G loss: 6.188311]\n","******* 623 [D loss: 0.138663, acc: 95.31%] [G loss: 6.068414]\n","******* 624 [D loss: 0.053266, acc: 100.00%] [G loss: 4.883985]\n","******* 625 [D loss: 0.070352, acc: 98.44%] [G loss: 4.742217]\n","******* 626 [D loss: 0.086626, acc: 96.88%] [G loss: 4.981853]\n","******* 627 [D loss: 0.048192, acc: 99.22%] [G loss: 6.565398]\n","******* 628 [D loss: 0.094034, acc: 93.75%] [G loss: 5.797532]\n","******* 629 [D loss: 0.065721, acc: 97.66%] [G loss: 4.857991]\n","******* 630 [D loss: 0.030127, acc: 100.00%] [G loss: 4.833087]\n","******* 631 [D loss: 0.045838, acc: 99.22%] [G loss: 5.085132]\n","******* 632 [D loss: 0.055926, acc: 99.22%] [G loss: 5.822863]\n","******* 633 [D loss: 0.093592, acc: 94.53%] [G loss: 5.041940]\n","******* 634 [D loss: 0.111294, acc: 95.31%] [G loss: 4.396118]\n","******* 635 [D loss: 0.141493, acc: 94.53%] [G loss: 4.374397]\n","******* 636 [D loss: 0.106484, acc: 96.88%] [G loss: 4.421965]\n","******* 637 [D loss: 0.069746, acc: 98.44%] [G loss: 4.709909]\n","******* 638 [D loss: 0.059986, acc: 99.22%] [G loss: 5.622756]\n","******* 639 [D loss: 0.071495, acc: 99.22%] [G loss: 5.003467]\n","******* 640 [D loss: 0.059819, acc: 99.22%] [G loss: 4.470997]\n","******* 641 [D loss: 0.108109, acc: 92.97%] [G loss: 4.972055]\n","******* 642 [D loss: 0.065232, acc: 98.44%] [G loss: 6.093542]\n","******* 643 [D loss: 0.110862, acc: 96.09%] [G loss: 5.279445]\n","******* 644 [D loss: 0.085382, acc: 97.66%] [G loss: 4.512568]\n","******* 645 [D loss: 0.064626, acc: 96.88%] [G loss: 4.653913]\n","******* 646 [D loss: 0.053964, acc: 99.22%] [G loss: 5.597672]\n","******* 647 [D loss: 0.049761, acc: 99.22%] [G loss: 6.063175]\n","******* 648 [D loss: 0.109079, acc: 96.88%] [G loss: 4.607011]\n","******* 649 [D loss: 0.103829, acc: 96.09%] [G loss: 4.178786]\n","******* 650 [D loss: 0.049509, acc: 99.22%] [G loss: 5.072021]\n","******* 651 [D loss: 0.069522, acc: 98.44%] [G loss: 5.460785]\n","******* 652 [D loss: 0.071920, acc: 96.88%] [G loss: 4.344404]\n","******* 653 [D loss: 0.097439, acc: 96.88%] [G loss: 5.285889]\n","******* 654 [D loss: 0.072739, acc: 97.66%] [G loss: 5.153425]\n","******* 655 [D loss: 0.094569, acc: 96.09%] [G loss: 5.276390]\n","******* 656 [D loss: 0.086687, acc: 96.09%] [G loss: 5.261529]\n","******* 657 [D loss: 0.064567, acc: 98.44%] [G loss: 5.740267]\n","******* 658 [D loss: 0.054885, acc: 99.22%] [G loss: 5.346703]\n","******* 659 [D loss: 0.156881, acc: 94.53%] [G loss: 5.173049]\n","******* 660 [D loss: 0.115328, acc: 96.09%] [G loss: 5.261182]\n","******* 661 [D loss: 0.081118, acc: 98.44%] [G loss: 5.492168]\n","******* 662 [D loss: 0.123071, acc: 95.31%] [G loss: 5.005963]\n","******* 663 [D loss: 0.107953, acc: 95.31%] [G loss: 4.743791]\n","******* 664 [D loss: 0.098371, acc: 96.09%] [G loss: 4.690554]\n","******* 665 [D loss: 0.056716, acc: 99.22%] [G loss: 5.520366]\n","******* 666 [D loss: 0.067137, acc: 97.66%] [G loss: 5.493561]\n","******* 667 [D loss: 0.085708, acc: 96.09%] [G loss: 4.927960]\n","******* 668 [D loss: 0.106940, acc: 96.88%] [G loss: 4.198175]\n","******* 669 [D loss: 0.100989, acc: 96.88%] [G loss: 5.095385]\n","******* 670 [D loss: 0.108229, acc: 97.66%] [G loss: 5.146241]\n","******* 671 [D loss: 0.130438, acc: 96.09%] [G loss: 4.968884]\n","******* 672 [D loss: 0.072025, acc: 100.00%] [G loss: 5.046214]\n","******* 673 [D loss: 0.061908, acc: 99.22%] [G loss: 5.221952]\n","******* 674 [D loss: 0.097266, acc: 96.88%] [G loss: 4.246543]\n","******* 675 [D loss: 0.101861, acc: 96.09%] [G loss: 4.392025]\n","******* 676 [D loss: 0.057191, acc: 98.44%] [G loss: 5.518849]\n","******* 677 [D loss: 0.066880, acc: 97.66%] [G loss: 5.369712]\n","******* 678 [D loss: 0.089688, acc: 96.88%] [G loss: 4.399180]\n","******* 679 [D loss: 0.076763, acc: 97.66%] [G loss: 4.601511]\n","******* 680 [D loss: 0.048328, acc: 100.00%] [G loss: 5.736030]\n","******* 681 [D loss: 0.172459, acc: 94.53%] [G loss: 4.326300]\n","******* 682 [D loss: 0.078544, acc: 99.22%] [G loss: 4.585999]\n","******* 683 [D loss: 0.068051, acc: 97.66%] [G loss: 4.737804]\n","******* 684 [D loss: 0.104077, acc: 97.66%] [G loss: 4.518005]\n","******* 685 [D loss: 0.065559, acc: 99.22%] [G loss: 5.094792]\n","******* 686 [D loss: 0.104053, acc: 97.66%] [G loss: 5.077780]\n","******* 687 [D loss: 0.079156, acc: 99.22%] [G loss: 4.416811]\n","******* 688 [D loss: 0.080729, acc: 97.66%] [G loss: 5.109648]\n","******* 689 [D loss: 0.091996, acc: 97.66%] [G loss: 5.092968]\n","******* 690 [D loss: 0.059546, acc: 100.00%] [G loss: 4.897550]\n","******* 691 [D loss: 0.134455, acc: 96.09%] [G loss: 4.170567]\n","******* 692 [D loss: 0.112362, acc: 95.31%] [G loss: 4.987293]\n","******* 693 [D loss: 0.110833, acc: 96.88%] [G loss: 5.211084]\n","******* 694 [D loss: 0.162417, acc: 95.31%] [G loss: 4.395998]\n","******* 695 [D loss: 0.084328, acc: 97.66%] [G loss: 4.651559]\n","******* 696 [D loss: 0.099712, acc: 96.88%] [G loss: 5.167738]\n","******* 697 [D loss: 0.052159, acc: 100.00%] [G loss: 4.894575]\n","******* 698 [D loss: 0.151369, acc: 95.31%] [G loss: 3.996046]\n","******* 699 [D loss: 0.114192, acc: 96.88%] [G loss: 4.621559]\n","******* 700 [D loss: 0.101284, acc: 94.53%] [G loss: 5.555444]\n","******* 701 [D loss: 0.101031, acc: 97.66%] [G loss: 5.605665]\n","******* 702 [D loss: 0.113495, acc: 96.09%] [G loss: 4.883543]\n","******* 703 [D loss: 0.091993, acc: 98.44%] [G loss: 3.986123]\n","******* 704 [D loss: 0.145462, acc: 92.97%] [G loss: 4.918851]\n","******* 705 [D loss: 0.038258, acc: 99.22%] [G loss: 6.074815]\n","******* 706 [D loss: 0.182093, acc: 90.62%] [G loss: 4.186502]\n","******* 707 [D loss: 0.159232, acc: 94.53%] [G loss: 3.113895]\n","******* 708 [D loss: 0.164300, acc: 93.75%] [G loss: 4.615922]\n","******* 709 [D loss: 0.057792, acc: 98.44%] [G loss: 6.487574]\n","******* 710 [D loss: 0.427636, acc: 78.91%] [G loss: 3.381162]\n","******* 711 [D loss: 0.269799, acc: 89.84%] [G loss: 3.246182]\n","******* 712 [D loss: 0.109929, acc: 96.09%] [G loss: 5.429565]\n","******* 713 [D loss: 0.086380, acc: 97.66%] [G loss: 6.879629]\n","******* 714 [D loss: 0.195107, acc: 92.19%] [G loss: 4.497943]\n","******* 715 [D loss: 0.242094, acc: 89.84%] [G loss: 3.709352]\n","******* 716 [D loss: 0.102385, acc: 95.31%] [G loss: 5.009369]\n","******* 717 [D loss: 0.066275, acc: 98.44%] [G loss: 6.239955]\n","******* 718 [D loss: 0.102481, acc: 97.66%] [G loss: 4.866203]\n","******* 719 [D loss: 0.174942, acc: 94.53%] [G loss: 3.784387]\n","******* 720 [D loss: 0.098393, acc: 96.88%] [G loss: 4.490873]\n","******* 721 [D loss: 0.133350, acc: 94.53%] [G loss: 5.027823]\n","******* 722 [D loss: 0.080011, acc: 96.88%] [G loss: 5.761715]\n","******* 723 [D loss: 0.116593, acc: 96.88%] [G loss: 4.538766]\n","******* 724 [D loss: 0.104918, acc: 96.88%] [G loss: 4.559268]\n","******* 725 [D loss: 0.162328, acc: 93.75%] [G loss: 4.699129]\n","******* 726 [D loss: 0.131837, acc: 95.31%] [G loss: 5.796960]\n","******* 727 [D loss: 0.174667, acc: 91.41%] [G loss: 4.791989]\n","******* 728 [D loss: 0.167219, acc: 94.53%] [G loss: 4.872983]\n","******* 729 [D loss: 0.085392, acc: 98.44%] [G loss: 5.289973]\n","******* 730 [D loss: 0.116124, acc: 95.31%] [G loss: 4.903100]\n","******* 731 [D loss: 0.101894, acc: 96.88%] [G loss: 4.399221]\n","******* 732 [D loss: 0.112375, acc: 95.31%] [G loss: 4.152163]\n","******* 733 [D loss: 0.106273, acc: 96.88%] [G loss: 4.405241]\n","******* 734 [D loss: 0.072946, acc: 99.22%] [G loss: 5.209162]\n","******* 735 [D loss: 0.090113, acc: 98.44%] [G loss: 5.355985]\n","******* 736 [D loss: 0.165162, acc: 95.31%] [G loss: 4.284345]\n","******* 737 [D loss: 0.133219, acc: 96.88%] [G loss: 3.893045]\n","******* 738 [D loss: 0.088496, acc: 96.88%] [G loss: 4.806072]\n","******* 739 [D loss: 0.097386, acc: 97.66%] [G loss: 5.292884]\n","******* 740 [D loss: 0.086842, acc: 98.44%] [G loss: 4.383499]\n","******* 741 [D loss: 0.211882, acc: 92.19%] [G loss: 3.663102]\n","******* 742 [D loss: 0.063987, acc: 98.44%] [G loss: 4.432850]\n","******* 743 [D loss: 0.143204, acc: 93.75%] [G loss: 5.377179]\n","******* 744 [D loss: 0.133302, acc: 96.88%] [G loss: 5.616149]\n","******* 745 [D loss: 0.301657, acc: 85.16%] [G loss: 3.186381]\n","******* 746 [D loss: 0.230354, acc: 90.62%] [G loss: 3.783933]\n","******* 747 [D loss: 0.118578, acc: 97.66%] [G loss: 5.923754]\n","******* 748 [D loss: 0.192689, acc: 89.84%] [G loss: 4.431528]\n","******* 749 [D loss: 0.199831, acc: 91.41%] [G loss: 4.084242]\n","******* 750 [D loss: 0.189887, acc: 93.75%] [G loss: 4.842575]\n","******* 751 [D loss: 0.134108, acc: 96.88%] [G loss: 5.184885]\n","******* 752 [D loss: 0.177311, acc: 92.19%] [G loss: 4.192321]\n","******* 753 [D loss: 0.264088, acc: 89.84%] [G loss: 3.944898]\n","******* 754 [D loss: 0.114041, acc: 96.88%] [G loss: 4.589690]\n","******* 755 [D loss: 0.108169, acc: 97.66%] [G loss: 4.537079]\n","******* 756 [D loss: 0.174235, acc: 91.41%] [G loss: 5.002279]\n","******* 757 [D loss: 0.076137, acc: 99.22%] [G loss: 4.418147]\n","******* 758 [D loss: 0.189330, acc: 93.75%] [G loss: 4.455653]\n","******* 759 [D loss: 0.145085, acc: 94.53%] [G loss: 3.722172]\n","******* 760 [D loss: 0.228042, acc: 90.62%] [G loss: 4.609753]\n","******* 761 [D loss: 0.145894, acc: 95.31%] [G loss: 5.806802]\n","******* 762 [D loss: 0.362242, acc: 83.59%] [G loss: 3.597691]\n","******* 763 [D loss: 0.261571, acc: 85.16%] [G loss: 4.075781]\n","******* 764 [D loss: 0.076279, acc: 98.44%] [G loss: 5.906482]\n","******* 765 [D loss: 0.280994, acc: 90.62%] [G loss: 5.578839]\n","******* 766 [D loss: 0.158851, acc: 95.31%] [G loss: 4.662298]\n","******* 767 [D loss: 0.210841, acc: 91.41%] [G loss: 3.433342]\n","******* 768 [D loss: 0.196883, acc: 91.41%] [G loss: 4.688899]\n","******* 769 [D loss: 0.114132, acc: 96.09%] [G loss: 4.516349]\n","******* 770 [D loss: 0.132465, acc: 95.31%] [G loss: 4.736231]\n","******* 771 [D loss: 0.162222, acc: 92.19%] [G loss: 3.593046]\n","******* 772 [D loss: 0.218195, acc: 89.06%] [G loss: 4.431299]\n","******* 773 [D loss: 0.118006, acc: 96.09%] [G loss: 5.503518]\n","******* 774 [D loss: 0.237144, acc: 92.19%] [G loss: 4.377221]\n","******* 775 [D loss: 0.208863, acc: 90.62%] [G loss: 4.246813]\n","******* 776 [D loss: 0.125781, acc: 95.31%] [G loss: 4.755079]\n","******* 777 [D loss: 0.143830, acc: 95.31%] [G loss: 5.421911]\n","******* 778 [D loss: 0.135660, acc: 94.53%] [G loss: 5.265572]\n","******* 779 [D loss: 0.127628, acc: 96.88%] [G loss: 4.619263]\n","******* 780 [D loss: 0.072053, acc: 96.88%] [G loss: 5.070692]\n","******* 781 [D loss: 0.092415, acc: 97.66%] [G loss: 4.904006]\n","******* 782 [D loss: 0.135910, acc: 94.53%] [G loss: 4.903976]\n","******* 783 [D loss: 0.159160, acc: 93.75%] [G loss: 4.053551]\n","******* 784 [D loss: 0.183799, acc: 92.19%] [G loss: 3.922932]\n","******* 785 [D loss: 0.131058, acc: 96.09%] [G loss: 4.728036]\n","******* 786 [D loss: 0.240111, acc: 92.19%] [G loss: 5.025310]\n","******* 787 [D loss: 0.156320, acc: 93.75%] [G loss: 4.763453]\n","******* 788 [D loss: 0.244368, acc: 88.28%] [G loss: 4.123132]\n","******* 789 [D loss: 0.205143, acc: 91.41%] [G loss: 4.523061]\n","******* 790 [D loss: 0.114445, acc: 96.09%] [G loss: 5.500566]\n","******* 791 [D loss: 0.293933, acc: 89.06%] [G loss: 3.201677]\n","******* 792 [D loss: 0.411563, acc: 81.25%] [G loss: 3.842096]\n","******* 793 [D loss: 0.123172, acc: 96.88%] [G loss: 5.488959]\n","******* 794 [D loss: 0.138535, acc: 95.31%] [G loss: 5.471444]\n","******* 795 [D loss: 0.150787, acc: 95.31%] [G loss: 4.428767]\n","******* 796 [D loss: 0.172725, acc: 95.31%] [G loss: 4.613312]\n","******* 797 [D loss: 0.149710, acc: 95.31%] [G loss: 4.565976]\n","******* 798 [D loss: 0.068358, acc: 98.44%] [G loss: 4.671979]\n","******* 799 [D loss: 0.091065, acc: 96.88%] [G loss: 4.597724]\n","******* 800 [D loss: 0.108367, acc: 96.09%] [G loss: 4.045140]\n","******* 801 [D loss: 0.081033, acc: 98.44%] [G loss: 4.457979]\n","******* 802 [D loss: 0.100936, acc: 97.66%] [G loss: 3.994719]\n","******* 803 [D loss: 0.144873, acc: 96.09%] [G loss: 3.539607]\n","******* 804 [D loss: 0.104271, acc: 96.88%] [G loss: 4.319962]\n","******* 805 [D loss: 0.100589, acc: 97.66%] [G loss: 4.740002]\n","******* 806 [D loss: 0.205540, acc: 90.62%] [G loss: 3.761109]\n","******* 807 [D loss: 0.172754, acc: 95.31%] [G loss: 3.709715]\n","******* 808 [D loss: 0.141884, acc: 93.75%] [G loss: 4.688513]\n","******* 809 [D loss: 0.092235, acc: 97.66%] [G loss: 4.811969]\n","******* 810 [D loss: 0.174101, acc: 93.75%] [G loss: 4.382469]\n","******* 811 [D loss: 0.135975, acc: 96.09%] [G loss: 3.835653]\n","******* 812 [D loss: 0.188056, acc: 92.97%] [G loss: 4.025862]\n","******* 813 [D loss: 0.081262, acc: 96.88%] [G loss: 4.746234]\n","******* 814 [D loss: 0.169043, acc: 96.09%] [G loss: 3.775304]\n","******* 815 [D loss: 0.219647, acc: 89.84%] [G loss: 4.612911]\n","******* 816 [D loss: 0.110038, acc: 96.88%] [G loss: 4.664971]\n","******* 817 [D loss: 0.151004, acc: 93.75%] [G loss: 3.997076]\n","******* 818 [D loss: 0.218023, acc: 88.28%] [G loss: 4.126897]\n","******* 819 [D loss: 0.113346, acc: 97.66%] [G loss: 5.024682]\n","******* 820 [D loss: 0.157315, acc: 93.75%] [G loss: 4.146241]\n","******* 821 [D loss: 0.144871, acc: 94.53%] [G loss: 3.697686]\n","******* 822 [D loss: 0.169166, acc: 91.41%] [G loss: 5.090409]\n","******* 823 [D loss: 0.144519, acc: 94.53%] [G loss: 5.121445]\n","******* 824 [D loss: 0.110236, acc: 98.44%] [G loss: 4.090844]\n","******* 825 [D loss: 0.200981, acc: 91.41%] [G loss: 4.013600]\n","******* 826 [D loss: 0.101112, acc: 97.66%] [G loss: 5.232067]\n","******* 827 [D loss: 0.127593, acc: 95.31%] [G loss: 5.391452]\n","******* 828 [D loss: 0.165643, acc: 92.97%] [G loss: 4.363816]\n","******* 829 [D loss: 0.131102, acc: 93.75%] [G loss: 4.591395]\n","******* 830 [D loss: 0.140219, acc: 93.75%] [G loss: 4.308934]\n","******* 831 [D loss: 0.140838, acc: 92.97%] [G loss: 4.425211]\n","******* 832 [D loss: 0.075327, acc: 97.66%] [G loss: 4.891232]\n","******* 833 [D loss: 0.195334, acc: 92.97%] [G loss: 3.960772]\n","******* 834 [D loss: 0.112724, acc: 94.53%] [G loss: 4.464604]\n","******* 835 [D loss: 0.181982, acc: 92.97%] [G loss: 4.722528]\n","******* 836 [D loss: 0.136226, acc: 92.97%] [G loss: 4.652871]\n","******* 837 [D loss: 0.176696, acc: 92.97%] [G loss: 3.773530]\n","******* 838 [D loss: 0.177381, acc: 92.19%] [G loss: 4.136223]\n","******* 839 [D loss: 0.088865, acc: 97.66%] [G loss: 4.962916]\n","******* 840 [D loss: 0.164566, acc: 92.19%] [G loss: 4.366247]\n","******* 841 [D loss: 0.129383, acc: 94.53%] [G loss: 3.506350]\n","******* 842 [D loss: 0.191686, acc: 94.53%] [G loss: 4.311694]\n","******* 843 [D loss: 0.130216, acc: 96.09%] [G loss: 4.724222]\n","******* 844 [D loss: 0.180784, acc: 92.97%] [G loss: 3.906837]\n","******* 845 [D loss: 0.290240, acc: 86.72%] [G loss: 4.038233]\n","******* 846 [D loss: 0.182895, acc: 93.75%] [G loss: 5.242636]\n","******* 847 [D loss: 0.084964, acc: 97.66%] [G loss: 5.367722]\n","******* 848 [D loss: 0.170384, acc: 92.97%] [G loss: 3.883942]\n","******* 849 [D loss: 0.122899, acc: 95.31%] [G loss: 4.079225]\n","******* 850 [D loss: 0.102828, acc: 97.66%] [G loss: 3.955516]\n","******* 851 [D loss: 0.130902, acc: 96.88%] [G loss: 4.413522]\n","******* 852 [D loss: 0.110574, acc: 97.66%] [G loss: 4.736235]\n","******* 853 [D loss: 0.159080, acc: 93.75%] [G loss: 3.888649]\n","******* 854 [D loss: 0.206241, acc: 94.53%] [G loss: 3.372159]\n","******* 855 [D loss: 0.133813, acc: 94.53%] [G loss: 4.218262]\n","******* 856 [D loss: 0.178742, acc: 92.97%] [G loss: 4.397348]\n","******* 857 [D loss: 0.188670, acc: 92.19%] [G loss: 3.758291]\n","******* 858 [D loss: 0.136285, acc: 94.53%] [G loss: 3.990187]\n","******* 859 [D loss: 0.110116, acc: 97.66%] [G loss: 4.367358]\n","******* 860 [D loss: 0.119114, acc: 96.88%] [G loss: 4.355729]\n","******* 861 [D loss: 0.155340, acc: 93.75%] [G loss: 3.995309]\n","******* 862 [D loss: 0.109050, acc: 96.09%] [G loss: 4.077398]\n","******* 863 [D loss: 0.139269, acc: 96.88%] [G loss: 4.397034]\n","******* 864 [D loss: 0.124256, acc: 95.31%] [G loss: 4.122510]\n","******* 865 [D loss: 0.186537, acc: 92.97%] [G loss: 4.481980]\n","******* 866 [D loss: 0.190130, acc: 92.97%] [G loss: 4.185940]\n","******* 867 [D loss: 0.198768, acc: 91.41%] [G loss: 4.566823]\n","******* 868 [D loss: 0.149111, acc: 92.97%] [G loss: 4.004017]\n","******* 869 [D loss: 0.212401, acc: 91.41%] [G loss: 3.821356]\n","******* 870 [D loss: 0.170456, acc: 92.97%] [G loss: 4.439171]\n","******* 871 [D loss: 0.200895, acc: 92.97%] [G loss: 4.904491]\n","******* 872 [D loss: 0.179442, acc: 92.97%] [G loss: 3.487226]\n","******* 873 [D loss: 0.189377, acc: 92.19%] [G loss: 3.862643]\n","******* 874 [D loss: 0.138986, acc: 92.97%] [G loss: 4.968963]\n","******* 875 [D loss: 0.147439, acc: 95.31%] [G loss: 4.595696]\n","******* 876 [D loss: 0.296970, acc: 89.06%] [G loss: 3.107209]\n","******* 877 [D loss: 0.190818, acc: 92.19%] [G loss: 3.514516]\n","******* 878 [D loss: 0.165898, acc: 93.75%] [G loss: 4.236042]\n","******* 879 [D loss: 0.173483, acc: 92.19%] [G loss: 4.580497]\n","******* 880 [D loss: 0.187406, acc: 92.19%] [G loss: 4.117757]\n","******* 881 [D loss: 0.179486, acc: 93.75%] [G loss: 3.253719]\n","******* 882 [D loss: 0.288515, acc: 87.50%] [G loss: 3.486193]\n","******* 883 [D loss: 0.137961, acc: 94.53%] [G loss: 4.213341]\n","******* 884 [D loss: 0.249543, acc: 90.62%] [G loss: 3.857596]\n","******* 885 [D loss: 0.311228, acc: 85.94%] [G loss: 4.652266]\n","******* 886 [D loss: 0.172204, acc: 92.97%] [G loss: 4.648132]\n","******* 887 [D loss: 0.217793, acc: 90.62%] [G loss: 3.964987]\n","******* 888 [D loss: 0.311731, acc: 85.94%] [G loss: 3.632826]\n","******* 889 [D loss: 0.235378, acc: 90.62%] [G loss: 4.710840]\n","******* 890 [D loss: 0.132870, acc: 96.09%] [G loss: 5.141629]\n","******* 891 [D loss: 0.247211, acc: 91.41%] [G loss: 4.189762]\n","******* 892 [D loss: 0.217579, acc: 89.84%] [G loss: 3.922099]\n","******* 893 [D loss: 0.253735, acc: 87.50%] [G loss: 4.153959]\n","******* 894 [D loss: 0.156105, acc: 96.09%] [G loss: 4.540837]\n","******* 895 [D loss: 0.163721, acc: 94.53%] [G loss: 4.132961]\n","******* 896 [D loss: 0.243714, acc: 88.28%] [G loss: 4.369125]\n","******* 897 [D loss: 0.206875, acc: 92.19%] [G loss: 4.859525]\n","******* 898 [D loss: 0.250456, acc: 89.84%] [G loss: 4.739164]\n","******* 899 [D loss: 0.202043, acc: 93.75%] [G loss: 3.935543]\n","******* 900 [D loss: 0.260098, acc: 88.28%] [G loss: 4.239546]\n","******* 901 [D loss: 0.154524, acc: 92.19%] [G loss: 4.935086]\n","******* 902 [D loss: 0.257819, acc: 89.06%] [G loss: 4.493453]\n","******* 903 [D loss: 0.258903, acc: 88.28%] [G loss: 3.862918]\n","******* 904 [D loss: 0.222495, acc: 88.28%] [G loss: 3.895892]\n","******* 905 [D loss: 0.237126, acc: 88.28%] [G loss: 4.291048]\n","******* 906 [D loss: 0.139192, acc: 94.53%] [G loss: 4.893364]\n","******* 907 [D loss: 0.209216, acc: 90.62%] [G loss: 4.069509]\n","******* 908 [D loss: 0.208737, acc: 90.62%] [G loss: 4.973122]\n","******* 909 [D loss: 0.172816, acc: 95.31%] [G loss: 4.744431]\n","******* 910 [D loss: 0.255817, acc: 92.19%] [G loss: 4.768997]\n","******* 911 [D loss: 0.224133, acc: 90.62%] [G loss: 3.818444]\n","******* 912 [D loss: 0.238517, acc: 91.41%] [G loss: 4.095344]\n","******* 913 [D loss: 0.138753, acc: 96.88%] [G loss: 4.768914]\n","******* 914 [D loss: 0.178649, acc: 92.97%] [G loss: 4.655749]\n","******* 915 [D loss: 0.198832, acc: 92.97%] [G loss: 4.459587]\n","******* 916 [D loss: 0.210615, acc: 89.84%] [G loss: 4.037999]\n","******* 917 [D loss: 0.211996, acc: 90.62%] [G loss: 4.787466]\n","******* 918 [D loss: 0.166745, acc: 94.53%] [G loss: 4.550831]\n","******* 919 [D loss: 0.391692, acc: 82.03%] [G loss: 3.313094]\n","******* 920 [D loss: 0.239880, acc: 88.28%] [G loss: 4.119309]\n","******* 921 [D loss: 0.116328, acc: 96.09%] [G loss: 5.907738]\n","******* 922 [D loss: 0.262090, acc: 86.72%] [G loss: 3.956492]\n","******* 923 [D loss: 0.263281, acc: 89.06%] [G loss: 4.340217]\n","******* 924 [D loss: 0.180796, acc: 92.97%] [G loss: 4.594873]\n","******* 925 [D loss: 0.152919, acc: 96.88%] [G loss: 4.362764]\n","******* 926 [D loss: 0.163198, acc: 93.75%] [G loss: 4.487982]\n","******* 927 [D loss: 0.216923, acc: 92.19%] [G loss: 4.283669]\n","******* 928 [D loss: 0.080606, acc: 98.44%] [G loss: 4.919714]\n","******* 929 [D loss: 0.213867, acc: 90.62%] [G loss: 4.212430]\n","******* 930 [D loss: 0.212917, acc: 92.19%] [G loss: 3.680629]\n","******* 931 [D loss: 0.133671, acc: 93.75%] [G loss: 4.143757]\n","******* 932 [D loss: 0.165700, acc: 94.53%] [G loss: 3.984781]\n","******* 933 [D loss: 0.202870, acc: 92.97%] [G loss: 3.782412]\n","******* 934 [D loss: 0.156984, acc: 94.53%] [G loss: 3.515647]\n","******* 935 [D loss: 0.192056, acc: 92.97%] [G loss: 4.010859]\n","******* 936 [D loss: 0.210012, acc: 93.75%] [G loss: 3.402629]\n","******* 937 [D loss: 0.285335, acc: 88.28%] [G loss: 3.486131]\n","******* 938 [D loss: 0.140454, acc: 98.44%] [G loss: 4.097783]\n","******* 939 [D loss: 0.196014, acc: 92.97%] [G loss: 3.137600]\n","******* 940 [D loss: 0.243715, acc: 90.62%] [G loss: 3.190289]\n","******* 941 [D loss: 0.268754, acc: 89.84%] [G loss: 4.442254]\n","******* 942 [D loss: 0.233213, acc: 90.62%] [G loss: 4.540645]\n","******* 943 [D loss: 0.391494, acc: 79.69%] [G loss: 4.142531]\n","******* 944 [D loss: 0.318197, acc: 86.72%] [G loss: 3.395696]\n","******* 945 [D loss: 0.255605, acc: 88.28%] [G loss: 3.796544]\n","******* 946 [D loss: 0.254452, acc: 89.06%] [G loss: 4.439866]\n","******* 947 [D loss: 0.164109, acc: 95.31%] [G loss: 5.105486]\n","******* 948 [D loss: 0.374760, acc: 81.25%] [G loss: 4.147431]\n","******* 949 [D loss: 0.224093, acc: 92.97%] [G loss: 4.093956]\n","******* 950 [D loss: 0.267644, acc: 91.41%] [G loss: 5.117157]\n","******* 951 [D loss: 0.237786, acc: 92.19%] [G loss: 5.277086]\n","******* 952 [D loss: 0.405328, acc: 84.38%] [G loss: 4.012256]\n","******* 953 [D loss: 0.247988, acc: 92.19%] [G loss: 3.481598]\n","******* 954 [D loss: 0.234653, acc: 88.28%] [G loss: 4.535241]\n","******* 955 [D loss: 0.209454, acc: 89.84%] [G loss: 4.539204]\n","******* 956 [D loss: 0.268892, acc: 89.06%] [G loss: 4.334846]\n","******* 957 [D loss: 0.257544, acc: 89.84%] [G loss: 3.646198]\n","******* 958 [D loss: 0.291142, acc: 89.06%] [G loss: 3.647053]\n","******* 959 [D loss: 0.279211, acc: 85.94%] [G loss: 3.744010]\n","******* 960 [D loss: 0.437513, acc: 83.59%] [G loss: 3.211974]\n","******* 961 [D loss: 0.356350, acc: 83.59%] [G loss: 3.220746]\n","******* 962 [D loss: 0.181771, acc: 92.19%] [G loss: 3.986983]\n","******* 963 [D loss: 0.175661, acc: 92.97%] [G loss: 4.145293]\n","******* 964 [D loss: 0.326231, acc: 82.81%] [G loss: 3.598781]\n","******* 965 [D loss: 0.182196, acc: 92.19%] [G loss: 3.997200]\n","******* 966 [D loss: 0.246610, acc: 90.62%] [G loss: 4.807373]\n","******* 967 [D loss: 0.195710, acc: 92.97%] [G loss: 5.145415]\n","******* 968 [D loss: 0.194496, acc: 92.19%] [G loss: 3.919318]\n","******* 969 [D loss: 0.170715, acc: 95.31%] [G loss: 3.604467]\n","******* 970 [D loss: 0.178654, acc: 94.53%] [G loss: 4.422189]\n","******* 971 [D loss: 0.121667, acc: 95.31%] [G loss: 5.216937]\n","******* 972 [D loss: 0.216241, acc: 93.75%] [G loss: 3.521846]\n","******* 973 [D loss: 0.216116, acc: 92.97%] [G loss: 3.224379]\n","******* 974 [D loss: 0.264019, acc: 85.94%] [G loss: 4.643244]\n","******* 975 [D loss: 0.180563, acc: 94.53%] [G loss: 5.527758]\n","******* 976 [D loss: 0.261951, acc: 89.84%] [G loss: 4.856123]\n","******* 977 [D loss: 0.192497, acc: 92.97%] [G loss: 3.229396]\n","******* 978 [D loss: 0.224708, acc: 91.41%] [G loss: 3.702136]\n","******* 979 [D loss: 0.263711, acc: 87.50%] [G loss: 5.047951]\n","******* 980 [D loss: 0.179003, acc: 92.19%] [G loss: 5.239232]\n","******* 981 [D loss: 0.261143, acc: 90.62%] [G loss: 3.417830]\n","******* 982 [D loss: 0.287914, acc: 85.94%] [G loss: 3.016556]\n","******* 983 [D loss: 0.190745, acc: 91.41%] [G loss: 4.318343]\n","******* 984 [D loss: 0.244953, acc: 90.62%] [G loss: 5.091527]\n","******* 985 [D loss: 0.293624, acc: 90.62%] [G loss: 3.581464]\n","******* 986 [D loss: 0.235076, acc: 89.06%] [G loss: 3.557091]\n","******* 987 [D loss: 0.318630, acc: 85.16%] [G loss: 4.193651]\n","******* 988 [D loss: 0.312375, acc: 86.72%] [G loss: 4.291261]\n","******* 989 [D loss: 0.283390, acc: 86.72%] [G loss: 4.414940]\n","******* 990 [D loss: 0.243083, acc: 89.06%] [G loss: 4.029222]\n","******* 991 [D loss: 0.230147, acc: 91.41%] [G loss: 3.973506]\n","******* 992 [D loss: 0.197796, acc: 93.75%] [G loss: 4.023713]\n","******* 993 [D loss: 0.196140, acc: 92.97%] [G loss: 3.799197]\n","******* 994 [D loss: 0.391244, acc: 85.94%] [G loss: 3.430928]\n","******* 995 [D loss: 0.292772, acc: 88.28%] [G loss: 3.497430]\n","******* 996 [D loss: 0.191332, acc: 89.84%] [G loss: 3.941578]\n","******* 997 [D loss: 0.204353, acc: 92.97%] [G loss: 3.534025]\n","******* 998 [D loss: 0.204786, acc: 92.19%] [G loss: 3.431220]\n","******* 999 [D loss: 0.293452, acc: 90.62%] [G loss: 3.803945]\n","******* 1000 [D loss: 0.161938, acc: 96.09%] [G loss: 3.686908]\n","******* 1001 [D loss: 0.216571, acc: 91.41%] [G loss: 3.261464]\n","******* 1002 [D loss: 0.293023, acc: 86.72%] [G loss: 3.867112]\n","******* 1003 [D loss: 0.241937, acc: 90.62%] [G loss: 3.672184]\n","******* 1004 [D loss: 0.234426, acc: 90.62%] [G loss: 3.694965]\n","******* 1005 [D loss: 0.207387, acc: 92.19%] [G loss: 4.410552]\n","******* 1006 [D loss: 0.204464, acc: 90.62%] [G loss: 3.610544]\n","******* 1007 [D loss: 0.314763, acc: 89.84%] [G loss: 3.416511]\n","******* 1008 [D loss: 0.365063, acc: 85.16%] [G loss: 4.094705]\n","******* 1009 [D loss: 0.229060, acc: 91.41%] [G loss: 4.268756]\n","******* 1010 [D loss: 0.333426, acc: 85.94%] [G loss: 3.545755]\n","******* 1011 [D loss: 0.219063, acc: 89.84%] [G loss: 3.507882]\n","******* 1012 [D loss: 0.179553, acc: 91.41%] [G loss: 4.626092]\n","******* 1013 [D loss: 0.205820, acc: 92.97%] [G loss: 4.626153]\n","******* 1014 [D loss: 0.286345, acc: 87.50%] [G loss: 3.540695]\n","******* 1015 [D loss: 0.534429, acc: 76.56%] [G loss: 3.739992]\n","******* 1016 [D loss: 0.138762, acc: 94.53%] [G loss: 5.002013]\n","******* 1017 [D loss: 0.316727, acc: 89.84%] [G loss: 3.818433]\n","******* 1018 [D loss: 0.159454, acc: 95.31%] [G loss: 3.549706]\n","******* 1019 [D loss: 0.174041, acc: 92.97%] [G loss: 3.514999]\n","******* 1020 [D loss: 0.195648, acc: 92.97%] [G loss: 4.427029]\n","******* 1021 [D loss: 0.224357, acc: 90.62%] [G loss: 4.144578]\n","******* 1022 [D loss: 0.273990, acc: 89.84%] [G loss: 3.259507]\n","******* 1023 [D loss: 0.307885, acc: 85.94%] [G loss: 3.362551]\n","******* 1024 [D loss: 0.180687, acc: 92.19%] [G loss: 4.025313]\n","******* 1025 [D loss: 0.390155, acc: 85.16%] [G loss: 3.481105]\n","******* 1026 [D loss: 0.192048, acc: 92.97%] [G loss: 3.759345]\n","******* 1027 [D loss: 0.299554, acc: 89.84%] [G loss: 4.195774]\n","******* 1028 [D loss: 0.269826, acc: 89.06%] [G loss: 3.823892]\n","******* 1029 [D loss: 0.255603, acc: 88.28%] [G loss: 3.527202]\n","******* 1030 [D loss: 0.196911, acc: 93.75%] [G loss: 4.162125]\n","******* 1031 [D loss: 0.204938, acc: 92.19%] [G loss: 4.594957]\n","******* 1032 [D loss: 0.247917, acc: 88.28%] [G loss: 3.803018]\n","******* 1033 [D loss: 0.257311, acc: 90.62%] [G loss: 3.517830]\n","******* 1034 [D loss: 0.257684, acc: 89.84%] [G loss: 3.716501]\n","******* 1035 [D loss: 0.202607, acc: 92.97%] [G loss: 4.354259]\n","******* 1036 [D loss: 0.184280, acc: 92.97%] [G loss: 4.071976]\n","******* 1037 [D loss: 0.173423, acc: 92.19%] [G loss: 3.481992]\n","******* 1038 [D loss: 0.423119, acc: 82.03%] [G loss: 3.100343]\n","******* 1039 [D loss: 0.337553, acc: 85.94%] [G loss: 3.940281]\n","******* 1040 [D loss: 0.188288, acc: 91.41%] [G loss: 4.616893]\n","******* 1041 [D loss: 0.243960, acc: 89.84%] [G loss: 4.087081]\n","******* 1042 [D loss: 0.242883, acc: 89.84%] [G loss: 3.376478]\n","******* 1043 [D loss: 0.231419, acc: 89.84%] [G loss: 3.423689]\n","******* 1044 [D loss: 0.226812, acc: 89.84%] [G loss: 3.723849]\n","******* 1045 [D loss: 0.257232, acc: 90.62%] [G loss: 4.477253]\n","******* 1046 [D loss: 0.242552, acc: 91.41%] [G loss: 3.956733]\n","******* 1047 [D loss: 0.241777, acc: 90.62%] [G loss: 3.301829]\n","******* 1048 [D loss: 0.284611, acc: 83.59%] [G loss: 3.573902]\n","******* 1049 [D loss: 0.183878, acc: 92.19%] [G loss: 4.969370]\n","******* 1050 [D loss: 0.284071, acc: 88.28%] [G loss: 4.171377]\n","******* 1051 [D loss: 0.380644, acc: 83.59%] [G loss: 2.973301]\n","******* 1052 [D loss: 0.320432, acc: 85.16%] [G loss: 3.217138]\n","******* 1053 [D loss: 0.198243, acc: 90.62%] [G loss: 4.279537]\n","******* 1054 [D loss: 0.196356, acc: 91.41%] [G loss: 5.037000]\n","******* 1055 [D loss: 0.391492, acc: 82.81%] [G loss: 3.530956]\n","******* 1056 [D loss: 0.364274, acc: 82.81%] [G loss: 3.558127]\n","******* 1057 [D loss: 0.293509, acc: 87.50%] [G loss: 4.289946]\n","******* 1058 [D loss: 0.147636, acc: 95.31%] [G loss: 4.618262]\n","******* 1059 [D loss: 0.279160, acc: 88.28%] [G loss: 3.955185]\n","******* 1060 [D loss: 0.439738, acc: 81.25%] [G loss: 2.765536]\n","******* 1061 [D loss: 0.199772, acc: 91.41%] [G loss: 4.122193]\n","******* 1062 [D loss: 0.145791, acc: 93.75%] [G loss: 4.862923]\n","******* 1063 [D loss: 0.419232, acc: 81.25%] [G loss: 3.290966]\n","******* 1064 [D loss: 0.366041, acc: 85.16%] [G loss: 3.022155]\n","******* 1065 [D loss: 0.196426, acc: 91.41%] [G loss: 4.082097]\n","******* 1066 [D loss: 0.175767, acc: 93.75%] [G loss: 4.282507]\n","******* 1067 [D loss: 0.330183, acc: 84.38%] [G loss: 3.686483]\n","******* 1068 [D loss: 0.226564, acc: 93.75%] [G loss: 3.392074]\n","******* 1069 [D loss: 0.202040, acc: 91.41%] [G loss: 3.942872]\n","******* 1070 [D loss: 0.214895, acc: 91.41%] [G loss: 4.215293]\n","******* 1071 [D loss: 0.350169, acc: 84.38%] [G loss: 3.869250]\n","******* 1072 [D loss: 0.204122, acc: 91.41%] [G loss: 3.673915]\n","******* 1073 [D loss: 0.128087, acc: 96.09%] [G loss: 4.090750]\n","******* 1074 [D loss: 0.301425, acc: 85.16%] [G loss: 4.112370]\n","******* 1075 [D loss: 0.251387, acc: 91.41%] [G loss: 3.809082]\n","******* 1076 [D loss: 0.329432, acc: 84.38%] [G loss: 3.505360]\n","******* 1077 [D loss: 0.156259, acc: 94.53%] [G loss: 4.046268]\n","******* 1078 [D loss: 0.216984, acc: 92.97%] [G loss: 4.117205]\n","******* 1079 [D loss: 0.499848, acc: 76.56%] [G loss: 2.875861]\n","******* 1080 [D loss: 0.515340, acc: 78.12%] [G loss: 3.530094]\n","******* 1081 [D loss: 0.290307, acc: 88.28%] [G loss: 4.390961]\n","******* 1082 [D loss: 0.296186, acc: 88.28%] [G loss: 4.243284]\n","******* 1083 [D loss: 0.424966, acc: 82.03%] [G loss: 3.041874]\n","******* 1084 [D loss: 0.490962, acc: 79.69%] [G loss: 2.966178]\n","******* 1085 [D loss: 0.192442, acc: 91.41%] [G loss: 4.060027]\n","******* 1086 [D loss: 0.409909, acc: 85.16%] [G loss: 4.023366]\n","******* 1087 [D loss: 0.356388, acc: 85.16%] [G loss: 3.037396]\n","******* 1088 [D loss: 0.315064, acc: 85.16%] [G loss: 3.354087]\n","******* 1089 [D loss: 0.394714, acc: 83.59%] [G loss: 3.629621]\n","******* 1090 [D loss: 0.361748, acc: 84.38%] [G loss: 3.414747]\n","******* 1091 [D loss: 0.312071, acc: 89.84%] [G loss: 4.051133]\n","******* 1092 [D loss: 0.496781, acc: 78.91%] [G loss: 2.639003]\n","******* 1093 [D loss: 0.378328, acc: 84.38%] [G loss: 2.828403]\n","******* 1094 [D loss: 0.265385, acc: 87.50%] [G loss: 3.816905]\n","******* 1095 [D loss: 0.226872, acc: 87.50%] [G loss: 4.980092]\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f45882d1b00>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n","    handle=self._handle, deleter=self._deleter)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n","    _ctx, \"DeleteIterator\", name, handle, deleter)\n","KeyboardInterrupt: \n"]},{"output_type":"stream","name":"stdout","text":["******* 1096 [D loss: 0.490618, acc: 78.91%] [G loss: 3.671897]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d8cd269f194d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-d8cd269f194d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#Generate Fake Images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#Train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1728\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 355\u001b[0;31m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1866\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m           preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   5022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5023\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5024\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5026\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[1;32m   3150\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3151\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[0;32m--> 924\u001b[0;31m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[1;32m    925\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[1;32m    926\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[0;34m(args, names, flat_shapes)\u001b[0m\n\u001b[1;32m   1158\u001b[0m   \u001b[0;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m   return _get_defun_inputs(\n\u001b[0;32m-> 1160\u001b[0;31m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[0;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[1;32m   1232\u001b[0m           placeholder = graph_placeholder(\n\u001b[1;32m   1233\u001b[0m               \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m               name=requested_name)\n\u001b[0m\u001b[1;32m   1235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m           \u001b[0;31m# Sometimes parameter names are not valid op names, so fall back to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m   op = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m     39\u001b[0m       \u001b[0;34m\"Placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       attrs=attrs, name=name)\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_invoke_op_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    599\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    600\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3567\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3570\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 2042\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   2043\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1881\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"po-jSQoN1Azl"},"source":["### **8) Making GIF**"]},{"cell_type":"code","metadata":{"id":"XPShgQpg1EMy"},"source":["# Display a single image using the epoch number\n","# def display_image(epoch_no):\n","#   return PIL.Image.open('generated_images/%.8f.png'.format(epoch_no))\n","\n","anim_file = 'dcgan.gif'\n","\n","with imageio.get_writer(anim_file, mode='I') as writer:\n","  filenames = glob.glob('generated_images/*.png')\n","  filenames = sorted(filenames)\n","  for filename in filenames:\n","    image = imageio.imread(filename)\n","    writer.append_data(image)\n","  image = imageio.imread(filename)\n","  writer.append_data(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wh37uv1torG5"},"source":[""],"execution_count":null,"outputs":[]}]}