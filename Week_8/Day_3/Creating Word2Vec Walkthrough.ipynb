{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b9f21c-dcb7-4d70-9d19-0db6aa2b5db3",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43eedadb-14d1-4770-8e32-e50f738ca59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n"
     ]
    }
   ],
   "source": [
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
    "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77969e63-c38e-43f2-ae0c-53f30eaf02c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943ae64f-2431-436f-9f8a-d0a81a5e6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'window_size': 2, # context window +- center word\n",
    "    'n': 10, # dimensions of word embeddings, also refer to size of hidden layer\n",
    "    'epochs': 50, # number of training epochs\n",
    "    'learning_rate': 0.01 # learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd0e60-5e0a-4551-80b5-f4447e5b96d2",
   "metadata": {},
   "source": [
    "# 3. Generate Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86aaf86-6d22-4207-9aa2-eb1ce7529375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class word2vec():\n",
    "  def __init__(self):\n",
    "    self.n = settings['n']\n",
    "    self.lr = settings['learning_rate']\n",
    "    self.epochs = settings['epochs']\n",
    "    self.window = settings['window_size']\n",
    "\n",
    "  def generate_training_data(self, settings, corpus):\n",
    "    # Find unique word counts using dictonary\n",
    "    word_counts = defaultdict(int)\n",
    "    for row in corpus:\n",
    "      for word in row:\n",
    "        word_counts[word] += 1\n",
    "    ## How many unique words in vocab? 9\n",
    "    self.v_count = len(word_counts.keys())\n",
    "    # Generate Lookup Dictionaries (vocab)\n",
    "    self.words_list = list(word_counts.keys())\n",
    "    # Generate word:index\n",
    "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "    # Generate index:word\n",
    "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\n",
    "    training_data = []\n",
    "    # Cycle through each sentence in corpus\n",
    "    for sentence in corpus:\n",
    "      sent_len = len(sentence)\n",
    "      # Cycle through each word in sentence\n",
    "      for i, word in enumerate(sentence):\n",
    "        # Convert target word to one-hot\n",
    "        w_target = self.word2onehot(sentence[i])\n",
    "        # Cycle through context window\n",
    "        w_context = []\n",
    "        # Note: window_size 2 will have range of 5 values\n",
    "        for j in range(i - self.window, i + self.window+1):\n",
    "          # Criteria for context word \n",
    "          # 1. Target word cannot be context word (j != i)\n",
    "          # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "          # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "          if j != i and j <= sent_len-1 and j >= 0:\n",
    "            # Append the one-hot representation of word to w_context\n",
    "            w_context.append(self.word2onehot(sentence[j]))\n",
    "            # print(sentence[i], sentence[j]) \n",
    "            # training_data contains a one-hot representation of the target word and context words\n",
    "        training_data.append([w_target, w_context])\n",
    "    return np.array(training_data)\n",
    "\n",
    "  def word2onehot(self, word):\n",
    "    # word_vec - initialise a blank vector\n",
    "    word_vec = [0 for i in range(0, self.v_count)] # Alternative - np.zeros(self.v_count)\n",
    "    # Get ID of word from word_index\n",
    "    word_index = self.word_index[word]\n",
    "    # Change value from 0 to 1 according to ID of the word\n",
    "    word_vec[word_index] = 1\n",
    "    return word_vec\n",
    "\n",
    "  def train(self, training_data):\n",
    "    # Initialising weight matrices\n",
    "    # Both s1 and s2 should be randomly initialised but for this demo, we pre-determine the arrays (getW1 and getW2)\n",
    "    # getW1 - shape (9x10) and getW2 - shape (10x9)\n",
    "#         self.w1 = np.array(getW1)\n",
    "#         self.w2 = np.array(getW2)\n",
    "    self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "    self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "    \n",
    "      ##Removed##\n",
    "  \n",
    "    # Cycle through each epoch\n",
    "    for i in range(self.epochs):\n",
    "      # Intialise loss to 0\n",
    "      self.loss = 0\n",
    "\n",
    "      # Cycle through each training sample\n",
    "      # w_t = vector for target word, w_c = vectors for context words\n",
    "      for w_t, w_c in training_data:\n",
    "        # Forward pass - Pass in vector for target word (w_t) to get:\n",
    "        # 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)\n",
    "        y_pred, h, u = self.forward_pass(w_t)\n",
    "\n",
    "\n",
    "              # Calculate error\n",
    "      # 1. For a target word, calculate difference between y_pred and each of the context words\n",
    "      # 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
    "        EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\n",
    "      # Backpropagation\n",
    "      # We use SGD to backpropagate errors - calculate loss on the output layer \n",
    "        self.backprop(EI, h, w_t)\n",
    "\n",
    "      # Calculate loss\n",
    "      # There are 2 parts to the loss function\n",
    "      # Part 1: -ve sum of all the output +\n",
    "      # Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
    "      # Note: word.index(1) returns the index in the context word vector with value 1\n",
    "      # Note: u[word.index(1)] returns the value of the output layer before softmax\n",
    "        self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "        #print('Epoch:', i, \"Loss:\", self.loss)\n",
    "        \n",
    "        ##Removed##\n",
    "    \n",
    "    return np.array(training_data)\n",
    "\n",
    "        \n",
    "  def forward_pass(self, x):\n",
    "    # x is one-hot vector for target word, shape - 9x1\n",
    "    # Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
    "    h = np.dot(self.w1.T, x)\n",
    "    # Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
    "    u = np.dot(self.w2.T, h)\n",
    "    # Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
    "    y_c = self.softmax(u)\n",
    "    return y_c, h, u\n",
    "  \n",
    "  def softmax(self, x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "  def backprop(self, e, h, x):\n",
    "    # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
    "    # Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
    "    # Going backwards, we need to take derivative of E with respect of w2\n",
    "    # h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
    "    dl_dw2 = np.outer(h, e)\n",
    "    # x - shape 1x8, w2 - 5x8, e.T - 8x1\n",
    "    # x - 1x8, np.dot() - 5x1, dl_dw1 - 8x5\n",
    "    dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "    # Update weights\n",
    "    self.w1 = self.w1 - (self.lr * dl_dw1)\n",
    "    self.w2 = self.w2 - (self.lr * dl_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad6a04d-6d37-40ee-8575-a7d99832b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\lhl\\lib\\site-packages\\ipykernel_launcher.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "# Initialise object\n",
    "w2v = word2vec()\n",
    "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "training_data = w2v.generate_training_data(settings, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc53cff-124f-46cb-8e2f-0ae516385156",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1ce8d8-acec-4ad9-be30-eae15de2bc99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 4.403464455608226\n",
      "Epoch: 0 Loss: 14.22888112209453\n",
      "Epoch: 0 Loss: 25.710650251832696\n",
      "Epoch: 0 Loss: 33.053651783239076\n",
      "Epoch: 0 Loss: 43.50295106433417\n",
      "Epoch: 0 Loss: 50.42062172290266\n",
      "Epoch: 0 Loss: 60.104306633568484\n",
      "Epoch: 0 Loss: 69.10682351499196\n",
      "Epoch: 0 Loss: 79.99040287137477\n",
      "Epoch: 0 Loss: 83.26320025523344\n",
      "Epoch: 1 Loss: 4.169039789614281\n",
      "Epoch: 1 Loss: 13.497212317398446\n",
      "Epoch: 1 Loss: 24.701941707874326\n",
      "Epoch: 1 Loss: 31.982941863912338\n",
      "Epoch: 1 Loss: 42.22144719775979\n",
      "Epoch: 1 Loss: 49.05208359046988\n",
      "Epoch: 1 Loss: 58.61860694639615\n",
      "Epoch: 1 Loss: 67.43858061085025\n",
      "Epoch: 1 Loss: 77.90250214153049\n",
      "Epoch: 1 Loss: 81.10019726160365\n",
      "Epoch: 2 Loss: 3.955467839657018\n",
      "Epoch: 2 Loss: 12.864266009701016\n",
      "Epoch: 2 Loss: 23.823603418049053\n",
      "Epoch: 2 Loss: 31.07386533765685\n",
      "Epoch: 2 Loss: 41.126001708977185\n",
      "Epoch: 2 Loss: 47.87807575488845\n",
      "Epoch: 2 Loss: 57.33671705459984\n",
      "Epoch: 2 Loss: 65.99123834378531\n",
      "Epoch: 2 Loss: 76.09066581253178\n",
      "Epoch: 2 Loss: 79.22027919608088\n",
      "Epoch: 3 Loss: 3.761362407912017\n",
      "Epoch: 3 Loss: 12.310698022710266\n",
      "Epoch: 3 Loss: 23.05076973273735\n",
      "Epoch: 3 Loss: 30.290815184180357\n",
      "Epoch: 3 Loss: 40.17638874122187\n",
      "Epoch: 3 Loss: 46.85741911622676\n",
      "Epoch: 3 Loss: 56.216163715089344\n",
      "Epoch: 3 Loss: 64.719861088397\n",
      "Epoch: 3 Loss: 74.49901795538374\n",
      "Epoch: 3 Loss: 77.56669284520224\n",
      "Epoch: 4 Loss: 3.5852123192292\n",
      "Epoch: 4 Loss: 11.822141849385709\n",
      "Epoch: 4 Loss: 22.36472562353929\n",
      "Epoch: 4 Loss: 29.60843799318065\n",
      "Epoch: 4 Loss: 39.34381104182645\n",
      "Epoch: 4 Loss: 45.96049902696939\n",
      "Epoch: 4 Loss: 55.22629985164029\n",
      "Epoch: 4 Loss: 63.591792054905426\n",
      "Epoch: 4 Loss: 73.08719860328151\n",
      "Epoch: 4 Loss: 76.0983425062375\n",
      "Epoch: 5 Loss: 3.425476662690635\n",
      "Epoch: 5 Loss: 11.387676596440128\n",
      "Epoch: 5 Loss: 21.751094224482635\n",
      "Epoch: 5 Loss: 29.00803053876783\n",
      "Epoch: 5 Loss: 38.606925023580736\n",
      "Epoch: 5 Loss: 45.16526226919489\n",
      "Epoch: 5 Loss: 54.34423422374807\n",
      "Epoch: 5 Loss: 62.58249351194236\n",
      "Epoch: 5 Loss: 71.82494811106048\n",
      "Epoch: 5 Loss: 74.78434566004671\n",
      "Epoch: 6 Loss: 3.280646984886617\n",
      "Epoch: 6 Loss: 10.998808052953448\n",
      "Epoch: 6 Loss: 21.19860858700745\n",
      "Epoch: 6 Loss: 28.475369154556216\n",
      "Epoch: 6 Loss: 37.949435259231706\n",
      "Epoch: 6 Loss: 44.45479252850973\n",
      "Epoch: 6 Loss: 53.55236004332701\n",
      "Epoch: 6 Loss: 61.673003906131285\n",
      "Epoch: 6 Loss: 70.68886654974277\n",
      "Epoch: 6 Loss: 73.60077012742616\n",
      "Epoch: 7 Loss: 3.1492866461434774\n",
      "Epoch: 7 Loss: 10.64879157613542\n",
      "Epoch: 7 Loss: 20.698278431753725\n",
      "Epoch: 7 Loss: 27.99936288165625\n",
      "Epoch: 7 Loss: 37.358593691629366\n",
      "Epoch: 7 Loss: 43.81579562249989\n",
      "Epoch: 7 Loss: 52.83680672354037\n",
      "Epoch: 7 Loss: 60.848332365779896\n",
      "Epoch: 7 Loss: 69.66040424980734\n",
      "Epoch: 7 Loss: 72.52860731778111\n",
      "Epoch: 8 Loss: 3.0300541772157947\n",
      "Epoch: 8 Loss: 10.33217626100745\n",
      "Epoch: 8 Loss: 20.242817422581055\n",
      "Epoch: 8 Loss: 27.571188693428823\n",
      "Epoch: 8 Loss: 36.824230539954016\n",
      "Epoch: 8 Loss: 43.23761951255627\n",
      "Epoch: 8 Loss: 52.186435427084746\n",
      "Epoch: 8 Loss: 60.09640839564343\n",
      "Epoch: 8 Loss: 68.7245663783869\n",
      "Epoch: 8 Loss: 71.55246331584536\n",
      "Epoch: 9 Loss: 2.9217153043397155\n",
      "Epoch: 9 Loss: 10.04449159375378\n",
      "Epoch: 9 Loss: 19.826241764878745\n",
      "Epoch: 9 Loss: 27.18371601822288\n",
      "Epoch: 9 Loss: 36.338105955730306\n",
      "Epoch: 9 Loss: 42.711597138189504\n",
      "Epoch: 9 Loss: 51.592163678255226\n",
      "Epoch: 9 Loss: 59.40736973293095\n",
      "Epoch: 9 Loss: 67.8690444590403\n",
      "Epoch: 9 Loss: 70.65967984700667\n",
      "Epoch: 10 Loss: 2.823146992708868\n",
      "Epoch: 10 Loss: 9.782026587511252\n",
      "Epoch: 10 Loss: 19.443582514280322\n",
      "Epoch: 10 Loss: 26.83111000025344\n",
      "Epoch: 10 Loss: 35.89346099317328\n",
      "Epoch: 10 Loss: 42.23059006490368\n",
      "Epoch: 10 Loss: 51.046495210225636\n",
      "Epoch: 10 Loss: 58.773062702045436\n",
      "Epoch: 10 Loss: 67.08361188881415\n",
      "Epoch: 10 Loss: 69.83972151004829\n",
      "Epoch: 11 Loss: 2.733336010644601\n",
      "Epoch: 11 Loss: 9.541669949967712\n",
      "Epoch: 11 Loss: 19.090674610216144\n",
      "Epoch: 11 Loss: 26.508548722431897\n",
      "Epoch: 11 Loss: 35.484696402565646\n",
      "Epoch: 11 Loss: 41.788661100938434\n",
      "Epoch: 11 Loss: 50.54318197647939\n",
      "Epoch: 11 Loss: 58.18668054220716\n",
      "Epoch: 11 Loss: 66.35968852611734\n",
      "Epoch: 11 Loss: 69.08373383714323\n",
      "Epoch: 12 Loss: 2.6513739117271027\n",
      "Epoch: 12 Loss: 9.320791433713376\n",
      "Epoch: 12 Loss: 18.763998764243162\n",
      "Epoch: 12 Loss: 26.212015511988405\n",
      "Epoch: 12 Loss: 35.107136097145\n",
      "Epoch: 12 Loss: 41.380832498098925\n",
      "Epoch: 12 Loss: 50.07697409509601\n",
      "Epoch: 12 Loss: 57.64249432698375\n",
      "Epoch: 12 Loss: 65.69001746639438\n",
      "Epoch: 12 Loss: 68.38421490035242\n",
      "Epoch: 13 Loss: 2.576449861797682\n",
      "Epoch: 13 Loss: 9.11715169724738\n",
      "Epoch: 13 Loss: 18.460560602020205\n",
      "Epoch: 13 Loss: 25.93814237335102\n",
      "Epoch: 13 Loss: 34.75684854610169\n",
      "Epoch: 13 Loss: 41.00290282264176\n",
      "Epoch: 13 Loss: 49.643430215445704\n",
      "Epoch: 13 Loss: 57.13564805599618\n",
      "Epoch: 13 Loss: 65.06841889179687\n",
      "Epoch: 13 Loss: 67.73476504233788\n",
      "Epoch: 14 Loss: 2.5078423577967284\n",
      "Epoch: 14 Loss: 8.928832442411704\n",
      "Epoch: 14 Loss: 18.177796680907825\n",
      "Epoch: 14 Loss: 25.684089367153867\n",
      "Epoch: 14 Loss: 34.43050903980088\n",
      "Epoch: 14 Loss: 40.65130531857086\n",
      "Epoch: 14 Loss: 49.23877070202079\n",
      "Epoch: 14 Loss: 56.66199959136073\n",
      "Epoch: 14 Loss: 64.48959863780391\n",
      "Epoch: 14 Loss: 67.12989213499131\n",
      "Epoch: 15 Loss: 2.444910578787817\n",
      "Epoch: 15 Loss: 8.75418135648193\n",
      "Epoch: 15 Loss: 17.913500332904075\n",
      "Epoch: 15 Loss: 25.447450024317526\n",
      "Epoch: 15 Loss: 34.12529162982895\n",
      "Epoch: 15 Loss: 40.32299647055578\n",
      "Epoch: 15 Loss: 48.859762031378054\n",
      "Epoch: 15 Loss: 56.21799525768107\n",
      "Epoch: 15 Loss: 63.948996781226484\n",
      "Epoch: 15 Loss: 66.56485748515904\n",
      "Epoch: 16 Loss: 2.387085867653524\n",
      "Epoch: 16 Loss: 8.591768128097453\n",
      "Epoch: 16 Loss: 17.66576243194848\n",
      "Epoch: 16 Loss: 25.2261761226656\n",
      "Epoch: 16 Loss: 33.838783168357864\n",
      "Epoch: 16 Loss: 40.01536711515341\n",
      "Epoch: 16 Loss: 48.50362452136996\n",
      "Epoch: 16 Loss: 55.80056976016644\n",
      "Epoch: 16 Loss: 63.44266627479361\n",
      "Epoch: 16 Loss: 66.03555226963235\n",
      "Epoch: 17 Loss: 2.333863657295498\n",
      "Epoch: 17 Loss: 8.440348922731278\n",
      "Epoch: 17 Loss: 17.43292359764703\n",
      "Epoch: 17 Loss: 25.018517193664607\n",
      "Epoch: 17 Loss: 33.56891416782192\n",
      "Epoch: 17 Loss: 39.72617075944238\n",
      "Epoch: 17 Loss: 48.16795788056726\n",
      "Epoch: 17 Loss: 55.407065532070675\n",
      "Epoch: 17 Loss: 62.96717464177971\n",
      "Epoch: 17 Loss: 65.53839740286472\n",
      "Epoch: 18 Loss: 2.2847960193559835\n",
      "Epoch: 18 Loss: 8.29883743537168\n",
      "Epoch: 18 Loss: 17.213535296010427\n",
      "Epoch: 18 Loss: 24.82297144711129\n",
      "Epoch: 18 Loss: 33.313902697229516\n",
      "Epoch: 18 Loss: 39.453465271683655\n",
      "Epoch: 18 Loss: 47.85068061340774\n",
      "Epoch: 18 Loss: 55.035167239538765\n",
      "Epoch: 18 Loss: 62.51952369014641\n",
      "Epoch: 18 Loss: 65.07026171269167\n",
      "Epoch: 19 Loss: 2.2394849175856923\n",
      "Epoch: 19 Loss: 8.166281130380868\n",
      "Epoch: 19 Loss: 17.006327949301866\n",
      "Epoch: 19 Loss: 24.638245680793574\n",
      "Epoch: 19 Loss: 33.07220853256099\n",
      "Epoch: 19 Loss: 39.1955651173959\n",
      "Epoch: 19 Loss: 47.54998035616768\n",
      "Epoch: 19 Loss: 54.68284826670743\n",
      "Epoch: 19 Loss: 62.09708351084814\n",
      "Epoch: 19 Loss: 64.6283946245894\n",
      "Epoch: 20 Loss: 2.1975761833616314\n",
      "Epoch: 20 Loss: 8.04184161888263\n",
      "Epoch: 20 Loss: 16.810184625318755\n",
      "Epoch: 20 Loss: 24.46322234519743\n",
      "Epoch: 20 Loss: 32.84249546819504\n",
      "Epoch: 20 Loss: 38.95100200965916\n",
      "Epoch: 20 Loss: 47.2642729370006\n",
      "Epoch: 20 Loss: 54.34832676659406\n",
      "Epoch: 20 Loss: 61.69753792683322\n",
      "Epoch: 20 Loss: 64.21037047245115\n",
      "Epoch: 21 Loss: 2.1587541897680316\n",
      "Epoch: 21 Loss: 7.924778366069962\n",
      "Epoch: 21 Loss: 16.62411920629652\n",
      "Epoch: 21 Loss: 24.296932359204767\n",
      "Epoch: 21 Loss: 32.62360018449775\n",
      "Epoch: 21 Loss: 38.71849233542708\n",
      "Epoch: 21 Loss: 46.99216846317705\n",
      "Epoch: 21 Loss: 54.030029409601156\n",
      "Epoch: 21 Loss: 61.318839200936495\n",
      "Epoch: 21 Loss: 63.81404220750595\n",
      "Epoch: 22 Loss: 2.122737176603922\n",
      "Epoch: 22 Loss: 7.814435097564292\n",
      "Epoch: 22 Loss: 16.447258180270865\n",
      "Epoch: 22 Loss: 24.138532581862464\n",
      "Epoch: 22 Loss: 32.41450642146773\n",
      "Epoch: 22 Loss: 38.496910078374036\n",
      "Epoch: 22 Loss: 46.73244311011524\n",
      "Epoch: 22 Loss: 53.72656136302956\n",
      "Epoch: 22 Loss: 60.959170280088394\n",
      "Epoch: 22 Loss: 63.43750275539029\n",
      "Epoch: 23 Loss: 2.089273166505115\n",
      "Epoch: 23 Loss: 7.710228406115772\n",
      "Epoch: 23 Loss: 16.27882537969212\n",
      "Epoch: 23 Loss: 23.98728707530983\n",
      "Epoch: 23 Loss: 32.21432347222202\n",
      "Epoch: 23 Loss: 38.28526422613743\n",
      "Epoch: 23 Loss: 46.48401556375331\n",
      "Epoch: 23 Loss: 53.43668133711767\n",
      "Epoch: 23 Loss: 60.6169132050064\n",
      "Epoch: 23 Loss: 63.07905263006238\n",
      "Epoch: 24 Loss: 2.0581364081242555\n",
      "Epoch: 24 Loss: 7.61163816076178\n",
      "Epoch: 24 Loss: 16.118129130796135\n",
      "Epoch: 24 Loss: 23.842551468757826\n",
      "Epoch: 24 Loss: 32.02226821041497\n",
      "Epoch: 24 Loss: 38.082679853173865\n",
      "Epoch: 24 Loss: 46.24592727852521\n",
      "Epoch: 24 Loss: 53.15928076502599\n",
      "Epoch: 24 Loss: 60.290622583928354\n",
      "Epoch: 24 Loss: 62.73717268784915\n",
      "Epoch: 25 Loss: 2.029124283102026\n",
      "Epoch: 25 Loss: 7.518199398748159\n",
      "Epoch: 25 Loss: 15.964551384431903\n",
      "Epoch: 25 Loss: 23.70375986861731\n",
      "Epoch: 25 Loss: 31.837650020318037\n",
      "Epoch: 25 Loss: 37.88838222783616\n",
      "Epoch: 25 Loss: 46.01732587625575\n",
      "Epoch: 25 Loss: 52.89336636517039\n",
      "Epoch: 25 Loss: 59.979003239005706\n",
      "Epoch: 25 Loss: 62.410501118838056\n",
      "Epoch: 26 Loss: 2.002054617326478\n",
      "Epoch: 26 Loss: 7.429495441924411\n",
      "Epoch: 26 Loss: 15.817538482810358\n",
      "Epoch: 26 Loss: 23.570413865928607\n",
      "Epoch: 26 Loss: 31.659858119321417\n",
      "Epoch: 26 Loss: 37.701683415817364\n",
      "Epoch: 26 Loss: 45.797451139233054\n",
      "Epoch: 26 Loss: 52.63804547743966\n",
      "Epoch: 26 Loss: 59.68089130015131\n",
      "Epoch: 26 Loss: 62.09781394183618\n",
      "Epoch: 27 Loss: 1.9767633423021858\n",
      "Epoch: 27 Loss: 7.345152028019913\n",
      "Epoch: 27 Loss: 15.676593282674698\n",
      "Epoch: 27 Loss: 23.44207327635055\n",
      "Epoch: 27 Loss: 31.48835085839257\n",
      "Epoch: 27 Loss: 37.521970950169354\n",
      "Epoch: 27 Loss: 45.5856231522325\n",
      "Epoch: 27 Loss: 52.39251367919071\n",
      "Epoch: 27 Loss: 59.39523815408453\n",
      "Epoch: 27 Loss: 61.798008404117766\n",
      "Epoch: 28 Loss: 1.953102458402049\n",
      "Epoch: 28 Loss: 7.264832286104448\n",
      "Epoch: 28 Loss: 15.54126840782153\n",
      "Epoch: 28 Loss: 23.318348315231447\n",
      "Epoch: 28 Loss: 31.322646662472195\n",
      "Epoch: 28 Loss: 37.34869821663129\n",
      "Epoch: 28 Loss: 45.38123222952813\n",
      "Epoch: 28 Loss: 52.156044278973106\n",
      "Epoch: 28 Loss: 59.12109676358514\n",
      "Epoch: 28 Loss: 61.51008879591367\n",
      "Epoch: 29 Loss: 1.9309382577620964\n",
      "Epoch: 29 Loss: 7.188232416772261\n",
      "Epoch: 29 Loss: 15.411160445762736\n",
      "Epoch: 29 Loss: 23.198892964422374\n",
      "Epoch: 29 Loss: 31.162316334216122\n",
      "Epoch: 29 Loss: 37.18137626631535\n",
      "Epoch: 29 Loss: 45.18373032842432\n",
      "Epoch: 29 Loss: 51.92797936043239\n",
      "Epoch: 29 Loss: 58.857609959047274\n",
      "Epoch: 29 Loss: 61.23315427770558\n",
      "Epoch: 30 Loss: 1.9101497702544972\n",
      "Epoch: 30 Loss: 7.115077962774237\n",
      "Epoch: 30 Loss: 15.285904936895271\n",
      "Epoch: 30 Loss: 23.083399331277214\n",
      "Epoch: 30 Loss: 31.00697649411361\n",
      "Epoch: 30 Loss: 37.019566819108654\n",
      "Epoch: 30 Loss: 44.99262370391184\n",
      "Epoch: 30 Loss: 51.70772210932895\n",
      "Epoch: 30 Loss: 58.60400037541091\n",
      "Epoch: 30 Loss: 60.9663883900886\n",
      "Epoch: 31 Loss: 1.8906274011503754\n",
      "Epoch: 31 Loss: 7.04512057619301\n",
      "Epoch: 31 Loss: 15.165172031582753\n",
      "Epoch: 31 Loss: 22.971592835824282\n",
      "Epoch: 31 Loss: 30.856283970264116\n",
      "Epoch: 31 Loss: 36.86287626290394\n",
      "Epoch: 31 Loss: 44.807466602236175\n",
      "Epoch: 31 Loss: 51.494730205816765\n",
      "Epoch: 31 Loss: 58.359561765574085\n",
      "Epoch: 31 Loss: 60.70904997448066\n",
      "Epoch: 32 Loss: 1.8722717336827168\n",
      "Epoch: 32 Loss: 6.978135204773119\n",
      "Epoch: 32 Loss: 15.048662712396496\n",
      "Epoch: 32 Loss: 22.86322809102468\n",
      "Epoch: 32 Loss: 30.709930983854367\n",
      "Epoch: 32 Loss: 36.71095048784955\n",
      "Epoch: 32 Loss: 44.627855826414816\n",
      "Epoch: 32 Loss: 51.28851010418161\n",
      "Epoch: 32 Loss: 58.12365146888898\n",
      "Epoch: 32 Loss: 60.46046528079108\n",
      "Epoch: 33 Loss: 1.8549924737290668\n",
      "Epoch: 33 Loss: 6.91391763344095\n",
      "Epoch: 33 Loss: 14.936105496466094\n",
      "Epoch: 33 Loss: 22.758085364635406\n",
      "Epoch: 33 Loss: 30.56764100311346\n",
      "Epoch: 33 Loss: 36.56347042269222\n",
      "Epoch: 33 Loss: 44.45342603558795\n",
      "Epoch: 33 Loss: 51.088612054841306\n",
      "Epoch: 33 Loss: 57.89568385225951\n",
      "Epoch: 33 Loss: 60.2200210773027\n",
      "Epoch: 34 Loss: 1.8387075172813772\n",
      "Epoch: 34 Loss: 6.8522823279796095\n",
      "Epoch: 34 Loss: 14.827253547277707\n",
      "Epoch: 34 Loss: 22.655967530486965\n",
      "Epoch: 34 Loss: 30.42916516039353\n",
      "Epoch: 34 Loss: 36.42014816313959\n",
      "Epoch: 34 Loss: 44.283845663740614\n",
      "Epoch: 34 Loss: 50.89462574992395\n",
      "Epoch: 34 Loss: 57.67512457323683\n",
      "Epoch: 34 Loss: 59.98715861006031\n",
      "Epoch: 35 Loss: 1.8233421243054782\n",
      "Epoch: 35 Loss: 6.793060536743907\n",
      "Epoch: 35 Loss: 14.721882136987315\n",
      "Epoch: 35 Loss: 22.556697432780325\n",
      "Epoch: 35 Loss: 30.294279144951965\n",
      "Epoch: 35 Loss: 36.280723600933605\n",
      "Epoch: 35 Loss: 44.1188133627654\n",
      "Epoch: 35 Loss: 50.70617649528111\n",
      "Epoch: 35 Loss: 57.46148554062402\n",
      "Epoch: 35 Loss: 59.761368285290004\n",
      "Epoch: 36 Loss: 1.8088281850732164\n",
      "Epoch: 36 Loss: 6.736098613595703\n",
      "Epoch: 36 Loss: 14.61978640990544\n",
      "Epoch: 36 Loss: 22.460115599957593\n",
      "Epoch: 36 Loss: 30.162780498734094\n",
      "Epoch: 36 Loss: 36.14496147774892\n",
      "Epoch: 36 Loss: 43.95805489081586\n",
      "Epoch: 36 Loss: 50.52292182926723\n",
      "Epoch: 36 Loss: 57.254320469480085\n",
      "Epoch: 36 Loss: 59.54218496984566\n",
      "Epoch: 37 Loss: 1.7951035671365432\n",
      "Epoch: 37 Loss: 6.681256531221415\n",
      "Epoch: 37 Loss: 14.520779405675011\n",
      "Epoch: 37 Loss: 22.366078255335474\n",
      "Epoch: 37 Loss: 30.034486254570645\n",
      "Epoch: 37 Loss: 36.01264880072633\n",
      "Epoch: 37 Loss: 43.801320380065434\n",
      "Epoch: 37 Loss: 50.34454852277951\n",
      "Epoch: 37 Loss: 57.05322094491549\n",
      "Epoch: 37 Loss: 59.329183822261584\n",
      "Epoch: 38 Loss: 1.7821115328618902\n",
      "Epoch: 38 Loss: 6.62840655891072\n",
      "Epoch: 38 Loss: 14.424690307136139\n",
      "Epoch: 38 Loss: 22.274455580435223\n",
      "Epoch: 38 Loss: 29.909230866189688\n",
      "Epoch: 38 Loss: 35.88359256691093\n",
      "Epoch: 38 Loss: 43.648381928841204\n",
      "Epoch: 38 Loss: 50.1707699065173\n",
      "Epoch: 38 Loss: 56.85781292339304\n",
      "Epoch: 38 Loss: 59.12197658138803\n",
      "Epoch: 39 Loss: 1.769800218907327\n",
      "Epoch: 39 Loss: 6.57743208293048\n",
      "Epoch: 39 Loss: 14.331362883216341\n",
      "Epoch: 39 Loss: 22.185130194146307\n",
      "Epoch: 39 Loss: 29.78686438768957\n",
      "Epoch: 39 Loss: 35.75761775249515\n",
      "Epoch: 39 Loss: 43.49903147206535\n",
      "Epoch: 39 Loss: 50.00132348069895\n",
      "Epoch: 39 Loss: 56.66775361197422\n",
      "Epoch: 39 Loss: 58.92020825138806\n",
      "Epoch: 40 Loss: 1.7581221702506307\n",
      "Epoch: 40 Loss: 6.528226550982604\n",
      "Epoch: 40 Loss: 14.240654101615187\n",
      "Epoch: 40 Loss: 22.097995816804456\n",
      "Epoch: 40 Loss: 29.667250866941814\n",
      "Epoch: 40 Loss: 35.63456552989496\n",
      "Epoch: 40 Loss: 43.353078891346925\n",
      "Epoch: 40 Loss: 49.83596876998267\n",
      "Epoch: 40 Loss: 56.48272867555589\n",
      "Epoch: 40 Loss: 58.72355413155768\n",
      "Epoch: 41 Loss: 1.7470339224032223\n",
      "Epoch: 41 Loss: 6.480692525020122\n",
      "Epoch: 41 Loss: 14.152432889739371\n",
      "Epoch: 41 Loss: 22.012956093178147\n",
      "Epoch: 41 Loss: 29.550266923048994\n",
      "Epoch: 41 Loss: 35.51429168158388\n",
      "Epoch: 41 Loss: 43.21035033220151\n",
      "Epoch: 41 Loss: 49.674485392420905\n",
      "Epoch: 41 Loss: 56.302449730021785\n",
      "Epoch: 41 Loss: 58.53171714739119\n",
      "Epoch: 42 Loss: 1.7364956263072724\n",
      "Epoch: 42 Loss: 6.434740829017761\n",
      "Epoch: 42 Loss: 14.066579025425826\n",
      "Epoch: 42 Loss: 21.92992355242817\n",
      "Epoch: 42 Loss: 29.435800482681014\n",
      "Epoch: 42 Loss: 35.3966651844953\n",
      "Epoch: 42 Loss: 43.07068670096336\n",
      "Epoch: 42 Loss: 49.51667131621443\n",
      "Epoch: 42 Loss: 56.12665208570955\n",
      "Epoch: 42 Loss: 58.3444254458744\n",
      "Epoch: 43 Loss: 1.7264707111391484\n",
      "Epoch: 43 Loss: 6.390289780236332\n",
      "Epoch: 43 Loss: 13.982982141576471\n",
      "Epoch: 43 Loss: 21.848818686484513\n",
      "Epoch: 43 Loss: 29.32374965402395\n",
      "Epoch: 43 Loss: 35.28156694285685\n",
      "Epoch: 43 Loss: 42.933942318179405\n",
      "Epoch: 43 Loss: 49.362341282048675\n",
      "Epoch: 43 Loss: 55.95509271092958\n",
      "Epoch: 43 Loss: 58.161430223409816\n",
      "Epoch: 44 Loss: 1.7169255808560946\n",
      "Epoch: 44 Loss: 6.347264494153574\n",
      "Epoch: 44 Loss: 13.901540831008973\n",
      "Epoch: 44 Loss: 21.769569131098592\n",
      "Epoch: 44 Loss: 29.21402172033869\n",
      "Epoch: 44 Loss: 35.16888865069268\n",
      "Epoch: 44 Loss: 42.79998370879079\n",
      "Epoch: 44 Loss: 49.211325372077425\n",
      "Epoch: 44 Loss: 55.787548389684645\n",
      "Epoch: 44 Loss: 57.982503759279055\n",
      "Epoch: 45 Loss: 1.7078293408435687\n",
      "Epoch: 45 Loss: 6.305596254612292\n",
      "Epoch: 45 Loss: 13.822161839674203\n",
      "Epoch: 45 Loss: 21.692108936177295\n",
      "Epoch: 45 Loss: 29.106532237854346\n",
      "Epoch: 45 Loss: 35.05853176803808\n",
      "Epoch: 45 Loss: 42.66868851233875\n",
      "Epoch: 45 Loss: 49.06346770931199\n",
      "Epoch: 45 Loss: 55.623814051400956\n",
      "Epoch: 45 Loss: 57.80743763129746\n",
      "Epoch: 46 Loss: 1.699153551464458\n",
      "Epoch: 46 Loss: 6.265221941904663\n",
      "Epoch: 46 Loss: 13.744759337961963\n",
      "Epoch: 46 Loss: 21.616377913971853\n",
      "Epoch: 46 Loss: 29.00120422500741\n",
      "Epoch: 46 Loss: 34.9504065972575\n",
      "Epoch: 46 Loss: 42.53994449888205\n",
      "Epoch: 46 Loss: 48.918625273399456\n",
      "Epoch: 46 Loss: 55.46370125353502\n",
      "Epoch: 46 Loss: 57.6360410934585\n",
      "Epoch: 47 Loss: 1.6908720056914337\n",
      "Epoch: 47 Loss: 6.226083512505272\n",
      "Epoch: 47 Loss: 13.669254261158416\n",
      "Epoch: 47 Loss: 21.54232105534578\n",
      "Epoch: 47 Loss: 28.897967431956786\n",
      "Epoch: 47 Loss: 34.84443144782105\n",
      "Epoch: 47 Loss: 42.41364867836447\n",
      "Epoch: 47 Loss: 48.776666820621024\n",
      "Epoch: 47 Loss: 55.30703680047862\n",
      "Epoch: 47 Loss: 57.46813959801052\n",
      "Epoch: 48 Loss: 1.6829605283306739\n",
      "Epoch: 48 Loss: 6.1881275250137735\n",
      "Epoch: 48 Loss: 13.595573711270038\n",
      "Epoch: 48 Loss: 21.469888005737907\n",
      "Epoch: 48 Loss: 28.796757680919857\n",
      "Epoch: 48 Loss: 34.740531879543816\n",
      "Epoch: 48 Loss: 42.28970649289418\n",
      "Epoch: 48 Loss: 48.6374718974909\n",
      "Epoch: 48 Loss: 55.15366148434048\n",
      "Epoch: 48 Loss: 57.30357344665076\n",
      "Epoch: 49 Loss: 1.675396794627873\n",
      "Epoch: 49 Loss: 6.151304707594484\n",
      "Epoch: 49 Loss: 13.523650413420313\n",
      "Epoch: 49 Loss: 21.3990325936129\n",
      "Epoch: 49 Loss: 28.697516269235724\n",
      "Epoch: 49 Loss: 34.638640015679115\n",
      "Epoch: 49 Loss: 42.16803108284609\n",
      "Epoch: 49 Loss: 48.500929938640034\n",
      "Epoch: 49 Loss: 55.00342893500717\n",
      "Epoch: 49 Loss: 57.142196557429095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[list([1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "        list([[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]])],\n",
       "       [list([0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       "        list([[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0]])],\n",
       "       [list([0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       "        list([[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0]])],\n",
       "       [list([0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "        list([[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0]])],\n",
       "       [list([0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       "        list([[0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0]])],\n",
       "       [list([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "        list([[0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0]])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "        list([[0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0]])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       "        list([[0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]])],\n",
       "       [list([0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "        list([[0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       "        list([[0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0]])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1dd8f-5da0-49ac-afac-d383669050f9",
   "metadata": {},
   "source": [
    "# 5. Inferencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24f28f3-bcb8-412f-ba3d-e1b9907d09ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'word2vec' object has no attribute 'word_vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6880/1199042556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get vector for word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"machine\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;31m## Removed ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'word2vec' object has no attribute 'word_vec'"
     ]
    }
   ],
   "source": [
    "# Get vector for word\n",
    "vec = w2v.word_vec(\"machine\")\n",
    "\n",
    "class word2vec():\n",
    "  ## Removed ##\n",
    "  \n",
    "  # Get vector from word\n",
    "  def word_vec(self, word):\n",
    "    w_index = self.word_index[word]\n",
    "    v_w = self.w1[w_index]\n",
    "    return v_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf305c2e-06d2-4adc-a287-a83161e2dbb9",
   "metadata": {},
   "source": [
    "# Gensim Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ceede15-69b6-47d2-b904-a522dd429dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'html.parser')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5953ac95-56bd-46a2-9bab-e628ccda54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516618f9-6cd7-4b42-ad3e-a4d88c1be796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b62d04-f7f1-43a2-be68-4bb8666e8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f650ec-e569-48a0-898f-87127a8a9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca01d0b-4b6c-4987-b430-932c0eee1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show words that appear at least twice\n",
    "vocabulary = word2vec.wv.vocab\n",
    "#print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd4a2806-9b3e-48f5-9436-db889e4d2634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.4275716e-03, -4.5426958e-03, -1.0319741e-03,  5.3739091e-03,\n",
       "        1.3346088e-03,  2.6749505e-03,  2.3683738e-03,  4.5259180e-03,\n",
       "        7.4983359e-04,  4.6483026e-04,  3.1671436e-03, -2.9444511e-03,\n",
       "        2.2884756e-03,  2.5514776e-03,  8.9689746e-04,  5.2988110e-03,\n",
       "        3.5735765e-03,  5.0671184e-03,  3.5097853e-03,  3.1882184e-04,\n",
       "        3.8150158e-03,  3.4713263e-03,  4.4909916e-03, -3.6571559e-03,\n",
       "        1.1781120e-03, -3.7262079e-03,  5.1380042e-03, -1.4140581e-04,\n",
       "       -1.6374971e-03,  2.8830691e-05,  3.3130441e-03,  5.5346108e-04,\n",
       "       -1.9603034e-03, -3.4262862e-03,  1.6257574e-03, -5.1931730e-03,\n",
       "       -1.8518420e-03, -3.8676381e-03,  4.2401184e-04,  2.5461868e-03,\n",
       "        3.9011026e-03, -6.9584360e-04,  5.6364303e-03,  3.8105107e-03,\n",
       "       -2.8897370e-03, -6.2121586e-03,  1.3195871e-03,  4.2332853e-03,\n",
       "       -1.8012434e-03, -1.8020233e-03,  1.8336568e-03, -2.1448592e-03,\n",
       "       -6.9063255e-03, -2.5056889e-03,  5.9005762e-03,  1.4972986e-03,\n",
       "        3.9780205e-03, -2.2348494e-03,  2.0800588e-04,  6.1498876e-03,\n",
       "        2.7715806e-03, -2.5894938e-03,  8.9866357e-05, -3.7413663e-03,\n",
       "        4.2887442e-03,  7.1088341e-04,  9.6156035e-04, -3.8155465e-04,\n",
       "        5.2777113e-04, -4.5902808e-03, -3.8715310e-03,  2.5042119e-03,\n",
       "       -4.3239226e-03,  6.3775494e-03, -2.1823882e-03, -5.3962425e-04,\n",
       "       -7.6118566e-04,  4.2190552e-03, -1.8637422e-03,  4.2825057e-03,\n",
       "        5.8223223e-03, -2.7094712e-03, -4.7816942e-03,  4.5031160e-03,\n",
       "        2.5754073e-03,  9.1161940e-04, -4.1634510e-03,  4.1454509e-03,\n",
       "        5.7682493e-05, -3.4786348e-04,  3.8883549e-03, -2.6638394e-03,\n",
       "        2.6248815e-03, -1.5497905e-03, -5.1150136e-03, -4.0954226e-03,\n",
       "        1.5049399e-03,  2.0722148e-03,  5.8641611e-03, -5.0583174e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the corresponding vector for a word\n",
    "v1 = word2vec.wv['artificial']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfc1e84-8087-4732-a976-6905d3f2809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine', 0.3812499940395355),\n",
       " ('ai', 0.3750268816947937),\n",
       " ('classical', 0.36905476450920105),\n",
       " ('hard', 0.3496473431587219),\n",
       " ('goals', 0.32975834608078003),\n",
       " ('problems', 0.32730886340141296),\n",
       " ('use', 0.3059532642364502),\n",
       " ('creating', 0.29804661870002747),\n",
       " ('assess', 0.2958163321018219),\n",
       " ('could', 0.2956176996231079)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920c18d-dc41-48ac-a538-2a24a604e597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
